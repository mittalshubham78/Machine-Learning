{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os                                                       # Miscellaneous operating system interfaces\n",
    "import re                                                       # Regular Expressions\n",
    "import random                                                   # Python Random Library\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize                         # Tokenizer\n",
    "from nltk.corpus import stopwords                               # Stop Words\n",
    "from nltk.stem.porter import *                                  # Stemmer - Porter\n",
    "from nltk.stem.snowball import SnowballStemmer                  # Stemmer - Snowball\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "from nltk.util import ngrams\n",
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['comp.graphics','rec.autos','sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20_newsgroups/comp.graphics'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"20_newsgroups\"\n",
    "os.path.join(datadir, categories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20_newsgroups/comp.graphics', '20_newsgroups/rec.autos', '20_newsgroups/sci.space']\n"
     ]
    }
   ],
   "source": [
    "datadir = \"20_newsgroups\"\n",
    "\n",
    "paths=[]\n",
    "l=[]\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(datadir, category)\n",
    "    paths.append(path)\n",
    "    \n",
    "print(paths)\n",
    "\n",
    "for path in paths:\n",
    "    for i in range(1):\n",
    "        choice = random.choice(os.listdir(path)) \n",
    "        fullfilename = os.path.join(path, choice)\n",
    "        with open(fullfilename) as f:\n",
    "            l.append(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!howland.reston.ans.net!noc.near.net!uunet!math.fu-berlin.de!irz401!sax!not-for-mail',\n",
       "  'From: joerg@sax.sax.de (Joerg Wunsch)',\n",
       "  'Newsgroups: comp.graphics',\n",
       "  'Subject: About the various DXF format questions',\n",
       "  'Date: 15 Apr 1993 14:18:43 +0200',\n",
       "  'Organization: SaxNet, Dresden, Germany',\n",
       "  'Lines: 25',\n",
       "  'Distribution: world',\n",
       "  'Message-ID: <1qjjr3$c30@sax.sax.de>',\n",
       "  'NNTP-Posting-Host: sax.sax.de',\n",
       "  'Summary: List of sites holding documentation of DXF format',\n",
       "  'Keywords: DXF, graphics formats',\n",
       "  '',\n",
       "  'Archie told me the following sites holding documentation about DXF:',\n",
       "  '',\n",
       "  'Host nic.funet.fi   (128.214.6.100)',\n",
       "  'Last updated 15:11  7 Apr 1993',\n",
       "  '',\n",
       "  '    Location: /pub/csc/graphics/format',\n",
       "  '      FILE      rwxrwxr--     95442  Dec  4  1991   dxf.doc',\n",
       "  '',\n",
       "  'Host rainbow.cse.nau.edu   (134.114.64.24)',\n",
       "  'Last updated 17:09  1 Jun 1992',\n",
       "  '',\n",
       "  '    Location: /graphics/formats',\n",
       "  '      FILE      rw-r--r--     95442  Mar 23 23:31   dxf.doc',\n",
       "  '',\n",
       "  'Host ftp.waseda.ac.jp   (133.9.1.32)',\n",
       "  'Last updated 00:47  5 Apr 1993',\n",
       "  '',\n",
       "  '    Location: /pub/data/graphic',\n",
       "  '      FILE      rw-r--r--     39753  Nov 18  1991   dxf.doc.Z',\n",
       "  '',\n",
       "  '-- ',\n",
       "  'J\"org Wunsch, ham: dl8dtl    : joerg_wunsch@uriah.sax.de',\n",
       "  'If anything can go wrong...  :   ...or:',\n",
       "  '     .o .o                   : joerg@sax.de,wutcd@hadrian.hrz.tu-chemnitz.de,',\n",
       "  '       <_      ... IT WILL!  : joerg_wunsch@tcd-dresden.de'],\n",
       " ['Newsgroups: rec.autos',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!howland.reston.ans.net!zaphod.mps.ohio-state.edu!uwm.edu!linac!att!cbnewsm!smartin',\n",
       "  'From: smartin@cbnewsm.cb.att.com (steven.c.martin)',\n",
       "  'Subject: Re: Car buying story, was: Christ, another dealer service scam...',\n",
       "  'Organization: AT&T',\n",
       "  'Distribution: usa',\n",
       "  'Date: Fri, 16 Apr 1993 18:50:24 GMT',\n",
       "  'Message-ID: <C5LB0A.JLE@cbnewsm.cb.att.com>',\n",
       "  'References: <1993Apr16.162950.25849@newsgate.sps.mot.com>',\n",
       "  'Lines: 36',\n",
       "  '',\n",
       "  'From article <1993Apr16.162950.25849@newsgate.sps.mot.com>, by markm@bigfoot.sps.mot.com (Mark Monninger):',\n",
       "  '',\n",
       "  \"> This kind of behavior is what I was shocked by in my 'experience'. For  \",\n",
       "  '> crying out loud, how do these turkeys think they can talk to customers  ',\n",
       "  \"> this way and still stay in business? Again, I don't expect sales people to  \",\n",
       "  \"> bow, scrape, and grovel in my presence but I sure don't expect to be  \",\n",
       "  '> abused either. I was very surprised by the way the sales people talked to  ',\n",
       "  \"> me and in other 'negotiating' sessions I overheard in neighboring sales  \",\n",
       "  '> cubicles. Evidently, their success rate is high enough that they continue  ',\n",
       "  '> to do business this way. There must be a lot of people out there who are  ',\n",
       "  '> easy to intimidate.',\n",
       "  '> ',\n",
       "  '',\n",
       "  'A couple of months ago I went to a dealership to test drive a car.  Afterwards,',\n",
       "  'we sat down to discuss prices.  I explained that I wanted a car just like the',\n",
       "  'one I drove, but in a different color.  He said he could get one exactly like',\n",
       "  'I wanted from the dealer network within a day.  We then negotiated a price and',\n",
       "  'signed the deal.',\n",
       "  '',\n",
       "  'Next day, I get a call.  He explains that they goofed, and they had neglected',\n",
       "  'to take into account a price increase. (The last price increase had occurred',\n",
       "  'over 4 months prior to my visit.)  If I still wanted the car, I would',\n",
       "  'have to fork over another $700.  As an alternative, they would honor the',\n",
       "  'price if I bought the car I test drove (which had been sitting around for ',\n",
       "  '6 months and had a few miles on it). I said goodbye.  This was a good',\n",
       "  \"example of how they can lowball you and still cover their butts.  It's too\",\n",
       "  \"bad more people don't demand honesty or these types of dealers would\",\n",
       "  'no longer be in business.  ',\n",
       "  '',\n",
       "  'The next dealership I went to was straightforward and honest.  First thing the',\n",
       "  'salesman said was, \"Lets\\'s see what you have for dealer cost and work out',\n",
       "  'how much profit I should make.\"  The deal went through with no problems.',\n",
       "  '',\n",
       "  '\\t\\t\\t\\t\\tSteve',\n",
       "  '',\n",
       "  ''],\n",
       " ['Newsgroups: sci.space',\n",
       "  'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!malgudi.oar.net!news.ans.net!europa.eng.gtefsd.com!howland.reston.ans.net!usc!cs.utexas.edu!utnut!utzoo!henry',\n",
       "  'From: henry@zoo.toronto.edu (Henry Spencer)',\n",
       "  'Subject: Re: Space Station Redesign, JSC Alternative #4',\n",
       "  'Message-ID: <C60H02.o7@zoo.toronto.edu>',\n",
       "  'Date: Sat, 24 Apr 1993 23:24:00 GMT',\n",
       "  'References: <1993Apr23.184732.1105@aio.jsc.nasa.gov> <23APR199317452695@tm0006.lerc.nasa.gov> <1ralibINNc0f@cbl.umd.edu>',\n",
       "  'Organization: U of Toronto Zoology',\n",
       "  'Lines: 14',\n",
       "  '',\n",
       "  'In article <1ralibINNc0f@cbl.umd.edu> mike@starburst.umd.edu (Michael F. Santangelo) writes:',\n",
       "  '>... The only thing',\n",
       "  \">that scares me is the part about simply strapping 3 SSME's and\",\n",
       "  '>a nosecone on it and \"just launching it.\"  I have this vision',\n",
       "  '>of something going terribly wrong with the launch resulting in the',\n",
       "  '>complete loss of the new modular space station (not just a peice of',\n",
       "  '>it as would be the case with staged in-orbit construction).',\n",
       "  '',\n",
       "  \"It doesn't make a whole lot of difference, actually, since they weren't\",\n",
       "  'building spares of the station hardware anyway.  (Dumb.)  At least this',\n",
       "  'is only one launch to fail.',\n",
       "  '-- ',\n",
       "  'SVR4 resembles a high-speed collision   | Henry Spencer @ U of Toronto Zoology',\n",
       "  'between SVR3 and SunOS.    - Dick Dunn  |  henry@zoo.toronto.edu  utzoo!henry']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting list of lists into a flat_list\n",
    "lines = [] \n",
    "for sublist in l:\n",
    "    for item in sublist:\n",
    "        lines.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!howland.reston.ans.net!noc.near.net!uunet!math.fu-berlin.de!irz401!sax!not-for-mail',\n",
       " 'From: joerg@sax.sax.de (Joerg Wunsch)',\n",
       " 'Newsgroups: comp.graphics',\n",
       " 'Subject: About the various DXF format questions',\n",
       " 'Date: 15 Apr 1993 14:18:43 +0200',\n",
       " 'Organization: SaxNet, Dresden, Germany',\n",
       " 'Lines: 25',\n",
       " 'Distribution: world',\n",
       " 'Message-ID: <1qjjr3$c30@sax.sax.de>',\n",
       " 'NNTP-Posting-Host: sax.sax.de',\n",
       " 'Summary: List of sites holding documentation of DXF format',\n",
       " 'Keywords: DXF, graphics formats',\n",
       " '',\n",
       " 'Archie told me the following sites holding documentation about DXF:',\n",
       " '',\n",
       " 'Host nic.funet.fi   (128.214.6.100)',\n",
       " 'Last updated 15:11  7 Apr 1993',\n",
       " '',\n",
       " '    Location: /pub/csc/graphics/format',\n",
       " '      FILE      rwxrwxr--     95442  Dec  4  1991   dxf.doc',\n",
       " '',\n",
       " 'Host rainbow.cse.nau.edu   (134.114.64.24)',\n",
       " 'Last updated 17:09  1 Jun 1992',\n",
       " '',\n",
       " '    Location: /graphics/formats',\n",
       " '      FILE      rw-r--r--     95442  Mar 23 23:31   dxf.doc',\n",
       " '',\n",
       " 'Host ftp.waseda.ac.jp   (133.9.1.32)',\n",
       " 'Last updated 00:47  5 Apr 1993',\n",
       " '',\n",
       " '    Location: /pub/data/graphic',\n",
       " '      FILE      rw-r--r--     39753  Nov 18  1991   dxf.doc.Z',\n",
       " '',\n",
       " '-- ',\n",
       " 'J\"org Wunsch, ham: dl8dtl    : joerg_wunsch@uriah.sax.de',\n",
       " 'If anything can go wrong...  :   ...or:',\n",
       " '     .o .o                   : joerg@sax.de,wutcd@hadrian.hrz.tu-chemnitz.de,',\n",
       " '       <_      ... IT WILL!  : joerg_wunsch@tcd-dresden.de',\n",
       " 'Newsgroups: rec.autos',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!gatech!howland.reston.ans.net!zaphod.mps.ohio-state.edu!uwm.edu!linac!att!cbnewsm!smartin',\n",
       " 'From: smartin@cbnewsm.cb.att.com (steven.c.martin)',\n",
       " 'Subject: Re: Car buying story, was: Christ, another dealer service scam...',\n",
       " 'Organization: AT&T',\n",
       " 'Distribution: usa',\n",
       " 'Date: Fri, 16 Apr 1993 18:50:24 GMT',\n",
       " 'Message-ID: <C5LB0A.JLE@cbnewsm.cb.att.com>',\n",
       " 'References: <1993Apr16.162950.25849@newsgate.sps.mot.com>',\n",
       " 'Lines: 36',\n",
       " '',\n",
       " 'From article <1993Apr16.162950.25849@newsgate.sps.mot.com>, by markm@bigfoot.sps.mot.com (Mark Monninger):',\n",
       " '',\n",
       " \"> This kind of behavior is what I was shocked by in my 'experience'. For  \",\n",
       " '> crying out loud, how do these turkeys think they can talk to customers  ',\n",
       " \"> this way and still stay in business? Again, I don't expect sales people to  \",\n",
       " \"> bow, scrape, and grovel in my presence but I sure don't expect to be  \",\n",
       " '> abused either. I was very surprised by the way the sales people talked to  ',\n",
       " \"> me and in other 'negotiating' sessions I overheard in neighboring sales  \",\n",
       " '> cubicles. Evidently, their success rate is high enough that they continue  ',\n",
       " '> to do business this way. There must be a lot of people out there who are  ',\n",
       " '> easy to intimidate.',\n",
       " '> ',\n",
       " '',\n",
       " 'A couple of months ago I went to a dealership to test drive a car.  Afterwards,',\n",
       " 'we sat down to discuss prices.  I explained that I wanted a car just like the',\n",
       " 'one I drove, but in a different color.  He said he could get one exactly like',\n",
       " 'I wanted from the dealer network within a day.  We then negotiated a price and',\n",
       " 'signed the deal.',\n",
       " '',\n",
       " 'Next day, I get a call.  He explains that they goofed, and they had neglected',\n",
       " 'to take into account a price increase. (The last price increase had occurred',\n",
       " 'over 4 months prior to my visit.)  If I still wanted the car, I would',\n",
       " 'have to fork over another $700.  As an alternative, they would honor the',\n",
       " 'price if I bought the car I test drove (which had been sitting around for ',\n",
       " '6 months and had a few miles on it). I said goodbye.  This was a good',\n",
       " \"example of how they can lowball you and still cover their butts.  It's too\",\n",
       " \"bad more people don't demand honesty or these types of dealers would\",\n",
       " 'no longer be in business.  ',\n",
       " '',\n",
       " 'The next dealership I went to was straightforward and honest.  First thing the',\n",
       " 'salesman said was, \"Lets\\'s see what you have for dealer cost and work out',\n",
       " 'how much profit I should make.\"  The deal went through with no problems.',\n",
       " '',\n",
       " '\\t\\t\\t\\t\\tSteve',\n",
       " '',\n",
       " '',\n",
       " 'Newsgroups: sci.space',\n",
       " 'Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!malgudi.oar.net!news.ans.net!europa.eng.gtefsd.com!howland.reston.ans.net!usc!cs.utexas.edu!utnut!utzoo!henry',\n",
       " 'From: henry@zoo.toronto.edu (Henry Spencer)',\n",
       " 'Subject: Re: Space Station Redesign, JSC Alternative #4',\n",
       " 'Message-ID: <C60H02.o7@zoo.toronto.edu>',\n",
       " 'Date: Sat, 24 Apr 1993 23:24:00 GMT',\n",
       " 'References: <1993Apr23.184732.1105@aio.jsc.nasa.gov> <23APR199317452695@tm0006.lerc.nasa.gov> <1ralibINNc0f@cbl.umd.edu>',\n",
       " 'Organization: U of Toronto Zoology',\n",
       " 'Lines: 14',\n",
       " '',\n",
       " 'In article <1ralibINNc0f@cbl.umd.edu> mike@starburst.umd.edu (Michael F. Santangelo) writes:',\n",
       " '>... The only thing',\n",
       " \">that scares me is the part about simply strapping 3 SSME's and\",\n",
       " '>a nosecone on it and \"just launching it.\"  I have this vision',\n",
       " '>of something going terribly wrong with the launch resulting in the',\n",
       " '>complete loss of the new modular space station (not just a peice of',\n",
       " '>it as would be the case with staged in-orbit construction).',\n",
       " '',\n",
       " \"It doesn't make a whole lot of difference, actually, since they weren't\",\n",
       " 'building spares of the station hardware anyway.  (Dumb.)  At least this',\n",
       " 'is only one launch to fail.',\n",
       " '-- ',\n",
       " 'SVR4 resembles a high-speed collision   | Henry Spencer @ U of Toronto Zoology',\n",
       " 'between SVR3 and SunOS.    - Dick Dunn  |  henry@zoo.toronto.edu  utzoo!henry']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'rochester', 'udel', 'gatech', 'howland', 'reston', 'ans', 'net', 'noc', 'near', 'net', 'uunet', 'math', 'fu', 'berlin', 'de', 'irz', 'sax', 'not', 'for', 'mail']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'rochester', 'udel', 'gatech', 'howland', 'reston', 'ans', 'net', 'noc', 'near', 'net', 'uunet', 'math', 'fu', 'berlin', 'de', 'irz', 'sax', 'mail']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'rochester', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'uunet', 'math', 'fu', 'berlin', 'de', 'irz', 'sax', 'mail']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rochest', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'uunet', 'math', 'fu', 'berlin', 'de', 'irz', 'sax', 'mail']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rochest', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'uunet', 'math', 'fu', 'berlin', 'de', 'irz', 'sax', 'mail']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('rochest', 'VBP'), ('udel', 'JJ'), ('gatech', 'NN'), ('howland', 'NN'), ('reston', 'VBZ'), ('an', 'DT'), ('net', 'JJ'), ('noc', 'NN'), ('near', 'IN'), ('net', 'JJ'), ('uunet', 'JJ'), ('math', 'NN'), ('fu', 'NN'), ('berlin', 'NN'), ('de', 'IN'), ('irz', 'FW'), ('sax', 'JJ'), ('mail', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN)\n",
      "  rochest/VBP\n",
      "  (mychunk udel/JJ)\n",
      "  (mychunk gatech/NN howland/NN)\n",
      "  reston/VBZ\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk noc/NN)\n",
      "  near/IN\n",
      "  (mychunk net/JJ uunet/JJ)\n",
      "  (mychunk math/NN fu/NN berlin/NN)\n",
      "  de/IN\n",
      "  irz/FW\n",
      "  (mychunk sax/JJ)\n",
      "  (mychunk mail/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rochest', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'noc', 'near', 'net', 'uunet', 'math', 'fu', 'berlin', 'de', 'irz', 'sax', 'mail']\n",
      "N_grams\n",
      "4-gram:  ['path cantaloup srv c', 'cantaloup srv c cmu', 'srv c cmu edu', 'c cmu edu rochest', 'cmu edu rochest udel', 'edu rochest udel gatech', 'rochest udel gatech howland', 'udel gatech howland reston', 'gatech howland reston an', 'howland reston an net', 'reston an net noc', 'an net noc near', 'net noc near net', 'noc near net uunet', 'near net uunet math', 'net uunet math fu', 'uunet math fu berlin', 'math fu berlin de', 'fu berlin de irz', 'berlin de irz sax', 'de irz sax mail']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'joerg', 'sax', 'sax', 'de', 'joerg', 'wunsch']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['joerg', 'sax', 'sax', 'de', 'joerg', 'wunsch']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['joerg', 'sax', 'sax', 'de', 'joerg', 'wunsch']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['joerg', 'sax', 'sax', 'de', 'joerg', 'wunsch']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['joerg', 'sax', 'sax', 'de', 'joerg', 'wunsch']\n",
      "After pos_tags: [('joerg', 'NN'), ('sax', 'NN'), ('sax', 'NN'), ('de', 'IN'), ('joerg', 'NN'), ('wunsch', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk joerg/NN sax/NN sax/NN)\n",
      "  de/IN\n",
      "  (mychunk joerg/NN wunsch/NN))\n",
      "\n",
      "\n",
      "['joerg', 'sax', 'sax', 'de', 'joerg', 'wunsch']\n",
      "N_grams\n",
      "4-gram:  ['joerg sax sax de', 'sax sax de joerg', 'sax de joerg wunsch']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['newsgroups', 'comp', 'graphics']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'comp', 'graphics']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'comp', 'graphic']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'comp', 'graphic']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'comp', 'graphic']\n",
      "After pos_tags: [('newsgroup', 'JJ'), ('comp', 'NN'), ('graphic', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk newsgroup/JJ) (mychunk comp/NN graphic/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'comp', 'graphic']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 'about', 'the', 'various', 'dxf', 'format', 'questions']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'various', 'dxf', 'format', 'questions']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'various', 'dxf', 'format', 'question']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'variou', 'dxf', 'format', 'question']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'variou', 'dxf', 'format', 'question']\n",
      "After pos_tags: [('subject', 'JJ'), ('variou', 'NN'), ('dxf', 'NN'), ('format', 'JJ'), ('question', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk subject/JJ)\n",
      "  (mychunk variou/NN dxf/NN format/JJ)\n",
      "  (mychunk question/NN))\n",
      "\n",
      "\n",
      "['subject', 'variou', 'dxf', 'format', 'question']\n",
      "N_grams\n",
      "4-gram:  ['subject variou dxf format', 'variou dxf format question']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'apr']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'apr']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'apr']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'apr']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'apr']\n",
      "After pos_tags: [('date', 'NN'), ('apr', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN apr/NN))\n",
      "\n",
      "\n",
      "['date', 'apr']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'saxnet', 'dresden', 'germany']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization', 'saxnet', 'dresden', 'germany']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization', 'saxnet', 'dresden', 'germany']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ', 'saxnet', 'dresden', 'germani']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ', 'saxnet', 'dresden', 'germani']\n",
      "After pos_tags: [('organ', 'JJ'), ('saxnet', 'NN'), ('dresden', 'NN'), ('germani', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk organ/JJ) (mychunk saxnet/NN dresden/NN germani/NN))\n",
      "\n",
      "\n",
      "['organ', 'saxnet', 'dresden', 'germani']\n",
      "N_grams\n",
      "4-gram:  ['organ saxnet dresden germani']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['distribution', 'world']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['distribut', 'world']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['distribut', 'world']\n",
      "After pos_tags: [('distribut', 'NN'), ('world', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk distribut/NN world/NN))\n",
      "\n",
      "\n",
      "['distribut', 'world']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'qjjr', 'c', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'qjjr', 'c', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'qjjr', 'c', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'qjjr', 'c', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'qjjr', 'c', 'sax', 'sax', 'de']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('qjjr', 'NN'), ('c', 'NN'), ('sax', 'JJ'), ('sax', 'NN'), ('de', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk messag/NN id/NN qjjr/NN c/NN sax/JJ)\n",
      "  (mychunk sax/NN)\n",
      "  de/IN)\n",
      "\n",
      "\n",
      "['messag', 'id', 'qjjr', 'c', 'sax', 'sax', 'de']\n",
      "N_grams\n",
      "4-gram:  ['messag id qjjr c', 'id qjjr c sax', 'qjjr c sax sax', 'c sax sax de']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['nntp', 'posting', 'host', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['nntp', 'posting', 'host', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['nntp', 'posting', 'host', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['nntp', 'post', 'host', 'sax', 'sax', 'de']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['nntp', 'post', 'host', 'sax', 'sax', 'de']\n",
      "After pos_tags: [('nntp', 'RB'), ('post', 'NN'), ('host', 'NN'), ('sax', 'NN'), ('sax', 'NN'), ('de', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S nntp/RB (mychunk post/NN host/NN sax/NN sax/NN) de/IN)\n",
      "\n",
      "\n",
      "['nntp', 'post', 'host', 'sax', 'sax', 'de']\n",
      "N_grams\n",
      "4-gram:  ['nntp post host sax', 'post host sax sax', 'host sax sax de']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['summary', 'list', 'of', 'sites', 'holding', 'documentation', 'of', 'dxf', 'format']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['summary', 'list', 'sites', 'holding', 'documentation', 'dxf', 'format']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['summary', 'list', 'site', 'holding', 'documentation', 'dxf', 'format']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['summari', 'list', 'site', 'hold', 'document', 'dxf', 'format']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['summari', 'list', 'site', 'hold', 'document', 'dxf', 'format']\n",
      "After pos_tags: [('summari', 'JJ'), ('list', 'NN'), ('site', 'NN'), ('hold', 'VBP'), ('document', 'NN'), ('dxf', 'NN'), ('format', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk summari/JJ)\n",
      "  (mychunk list/NN site/NN)\n",
      "  hold/VBP\n",
      "  (mychunk document/NN dxf/NN format/NN))\n",
      "\n",
      "\n",
      "['summari', 'list', 'site', 'hold', 'document', 'dxf', 'format']\n",
      "N_grams\n",
      "4-gram:  ['summari list site hold', 'list site hold document', 'site hold document dxf', 'hold document dxf format']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['keywords', 'dxf', 'graphics', 'formats']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['keywords', 'dxf', 'graphics', 'formats']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['keywords', 'dxf', 'graphic', 'format']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['keyword', 'dxf', 'graphic', 'format']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['keyword', 'dxf', 'graphic', 'format']\n",
      "After pos_tags: [('keyword', 'NN'), ('dxf', 'NN'), ('graphic', 'JJ'), ('format', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk keyword/NN dxf/NN graphic/JJ) (mychunk format/NN))\n",
      "\n",
      "\n",
      "['keyword', 'dxf', 'graphic', 'format']\n",
      "N_grams\n",
      "4-gram:  ['keyword dxf graphic format']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['archie', 'told', 'me', 'the', 'following', 'sites', 'holding', 'documentation', 'about', 'dxf']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['archie', 'told', 'following', 'sites', 'holding', 'documentation', 'dxf']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['archie', 'told', 'following', 'site', 'holding', 'documentation', 'dxf']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['archi', 'told', 'follow', 'site', 'hold', 'document', 'dxf']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['archi', 'told', 'follow', 'site', 'hold', 'document', 'dxf']\n",
      "After pos_tags: [('archi', 'NNS'), ('told', 'VBD'), ('follow', 'JJ'), ('site', 'NN'), ('hold', 'NN'), ('document', 'NN'), ('dxf', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk archi/NNS told/VBD follow/JJ)\n",
      "  (mychunk site/NN hold/NN document/NN dxf/NN))\n",
      "\n",
      "\n",
      "['archi', 'told', 'follow', 'site', 'hold', 'document', 'dxf']\n",
      "N_grams\n",
      "4-gram:  ['archi told follow site', 'told follow site hold', 'follow site hold document', 'site hold document dxf']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['host', 'nic', 'funet', 'fi']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['host', 'nic', 'funet', 'fi']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['host', 'nic', 'funet', 'fi']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['host', 'nic', 'funet', 'fi']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['host', 'nic', 'funet', 'fi']\n",
      "After pos_tags: [('host', 'NN'), ('nic', 'JJ'), ('funet', 'NN'), ('fi', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk host/NN nic/JJ) (mychunk funet/NN fi/NN))\n",
      "\n",
      "\n",
      "['host', 'nic', 'funet', 'fi']\n",
      "N_grams\n",
      "4-gram:  ['host nic funet fi']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['last', 'updated', 'apr']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['last', 'updated', 'apr']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['last', 'updated', 'apr']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['last', 'updat', 'apr']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['last', 'updat', 'apr']\n",
      "After pos_tags: [('last', 'JJ'), ('updat', 'JJ'), ('apr', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk last/JJ updat/JJ) (mychunk apr/NN))\n",
      "\n",
      "\n",
      "['last', 'updat', 'apr']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['location', 'pub', 'csc', 'graphics', 'format']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['location', 'pub', 'csc', 'graphics', 'format']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['location', 'pub', 'csc', 'graphic', 'format']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['locat', 'pub', 'csc', 'graphic', 'format']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['locat', 'pub', 'csc', 'graphic', 'format']\n",
      "After pos_tags: [('locat', 'NN'), ('pub', 'NN'), ('csc', 'NN'), ('graphic', 'JJ'), ('format', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk locat/NN pub/NN csc/NN graphic/JJ) (mychunk format/NN))\n",
      "\n",
      "\n",
      "['locat', 'pub', 'csc', 'graphic', 'format']\n",
      "N_grams\n",
      "4-gram:  ['locat pub csc graphic', 'pub csc graphic format']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['file', 'rwxrwxr', 'dec', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['file', 'rwxrwxr', 'dec', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['file', 'rwxrwxr', 'dec', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['file', 'rwxrwxr', 'dec', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['file', 'rwxrwxr', 'dec', 'dxf', 'doc']\n",
      "After pos_tags: [('file', 'NN'), ('rwxrwxr', 'NN'), ('dec', 'NN'), ('dxf', 'NN'), ('doc', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk file/NN rwxrwxr/NN dec/NN dxf/NN doc/NN))\n",
      "\n",
      "\n",
      "['file', 'rwxrwxr', 'dec', 'dxf', 'doc']\n",
      "N_grams\n",
      "4-gram:  ['file rwxrwxr dec dxf', 'rwxrwxr dec dxf doc']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['host', 'rainbow', 'cse', 'nau', 'edu']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['host', 'rainbow', 'cse', 'nau', 'edu']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['host', 'rainbow', 'cse', 'nau', 'edu']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['host', 'rainbow', 'cse', 'nau', 'edu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['host', 'rainbow', 'cse', 'nau', 'edu']\n",
      "After pos_tags: [('host', 'NN'), ('rainbow', 'NN'), ('cse', 'NN'), ('nau', 'NN'), ('edu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk host/NN rainbow/NN cse/NN nau/NN edu/NN))\n",
      "\n",
      "\n",
      "['host', 'rainbow', 'cse', 'nau', 'edu']\n",
      "N_grams\n",
      "4-gram:  ['host rainbow cse nau', 'rainbow cse nau edu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['last', 'updated', 'jun']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['last', 'updated', 'jun']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['last', 'updated', 'jun']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['last', 'updat', 'jun']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['last', 'updat', 'jun']\n",
      "After pos_tags: [('last', 'JJ'), ('updat', 'JJ'), ('jun', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk last/JJ updat/JJ) (mychunk jun/NN))\n",
      "\n",
      "\n",
      "['last', 'updat', 'jun']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['location', 'graphics', 'formats']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['location', 'graphics', 'formats']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['location', 'graphic', 'format']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['locat', 'graphic', 'format']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['locat', 'graphic', 'format']\n",
      "After pos_tags: [('locat', 'NN'), ('graphic', 'JJ'), ('format', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk locat/NN graphic/JJ) (mychunk format/NN))\n",
      "\n",
      "\n",
      "['locat', 'graphic', 'format']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['file', 'rw', 'r', 'r', 'mar', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['file', 'rw', 'r', 'r', 'mar', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['file', 'rw', 'r', 'r', 'mar', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['file', 'rw', 'r', 'r', 'mar', 'dxf', 'doc']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['file', 'rw', 'r', 'r', 'mar', 'dxf', 'doc']\n",
      "After pos_tags: [('file', 'NN'), ('rw', 'NN'), ('r', 'NN'), ('r', 'NN'), ('mar', 'NN'), ('dxf', 'NN'), ('doc', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk file/NN rw/NN r/NN r/NN mar/NN dxf/NN doc/NN))\n",
      "\n",
      "\n",
      "['file', 'rw', 'r', 'r', 'mar', 'dxf', 'doc']\n",
      "N_grams\n",
      "4-gram:  ['file rw r r', 'rw r r mar', 'r r mar dxf', 'r mar dxf doc']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['host', 'ftp', 'waseda', 'ac', 'jp']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['host', 'ftp', 'waseda', 'ac', 'jp']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['host', 'ftp', 'waseda', 'ac', 'jp']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['host', 'ftp', 'waseda', 'ac', 'jp']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['host', 'ftp', 'waseda', 'ac', 'jp']\n",
      "After pos_tags: [('host', 'NN'), ('ftp', 'NN'), ('waseda', 'NN'), ('ac', 'NN'), ('jp', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk host/NN ftp/NN waseda/NN ac/NN jp/NN))\n",
      "\n",
      "\n",
      "['host', 'ftp', 'waseda', 'ac', 'jp']\n",
      "N_grams\n",
      "4-gram:  ['host ftp waseda ac', 'ftp waseda ac jp']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['last', 'updated', 'apr']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['last', 'updated', 'apr']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['last', 'updated', 'apr']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['last', 'updat', 'apr']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['last', 'updat', 'apr']\n",
      "After pos_tags: [('last', 'JJ'), ('updat', 'JJ'), ('apr', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk last/JJ updat/JJ) (mychunk apr/NN))\n",
      "\n",
      "\n",
      "['last', 'updat', 'apr']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['location', 'pub', 'data', 'graphic']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['location', 'pub', 'data', 'graphic']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['location', 'pub', 'data', 'graphic']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['locat', 'pub', 'data', 'graphic']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['locat', 'pub', 'data', 'graphic']\n",
      "After pos_tags: [('locat', 'NN'), ('pub', 'NN'), ('data', 'NNS'), ('graphic', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk locat/NN pub/NN data/NNS graphic/NN))\n",
      "\n",
      "\n",
      "['locat', 'pub', 'data', 'graphic']\n",
      "N_grams\n",
      "4-gram:  ['locat pub data graphic']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['file', 'rw', 'r', 'r', 'nov', 'dxf', 'doc', 'z']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['file', 'rw', 'r', 'r', 'nov', 'dxf', 'doc', 'z']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['file', 'rw', 'r', 'r', 'nov', 'dxf', 'doc', 'z']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['file', 'rw', 'r', 'r', 'nov', 'dxf', 'doc', 'z']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['file', 'rw', 'r', 'r', 'nov', 'dxf', 'doc', 'z']\n",
      "After pos_tags: [('file', 'NN'), ('rw', 'NN'), ('r', 'NN'), ('r', 'NN'), ('nov', 'JJ'), ('dxf', 'NN'), ('doc', 'NN'), ('z', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk file/NN rw/NN r/NN r/NN nov/JJ)\n",
      "  (mychunk dxf/NN doc/NN z/NN))\n",
      "\n",
      "\n",
      "['file', 'rw', 'r', 'r', 'nov', 'dxf', 'doc', 'z']\n",
      "N_grams\n",
      "4-gram:  ['file rw r r', 'rw r r nov', 'r r nov dxf', 'r nov dxf doc', 'nov dxf doc z']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['j', 'org', 'wunsch', 'ham', 'dl', 'dtl', 'joerg', 'wunsch', 'uriah', 'sax', 'de']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['j', 'org', 'wunsch', 'ham', 'dl', 'dtl', 'joerg', 'wunsch', 'uriah', 'sax', 'de']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['j', 'org', 'wunsch', 'ham', 'dl', 'dtl', 'joerg', 'wunsch', 'uriah', 'sax', 'de']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['j', 'org', 'wunsch', 'ham', 'dl', 'dtl', 'joerg', 'wunsch', 'uriah', 'sax', 'de']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['j', 'org', 'wunsch', 'ham', 'dl', 'dtl', 'joerg', 'wunsch', 'uriah', 'sax', 'de']\n",
      "After pos_tags: [('j', 'NN'), ('org', 'MD'), ('wunsch', 'VB'), ('ham', 'NN'), ('dl', 'NN'), ('dtl', 'NN'), ('joerg', 'NN'), ('wunsch', 'NN'), ('uriah', 'JJ'), ('sax', 'NN'), ('de', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk j/NN)\n",
      "  org/MD\n",
      "  wunsch/VB\n",
      "  (mychunk ham/NN dl/NN dtl/NN joerg/NN wunsch/NN uriah/JJ)\n",
      "  (mychunk sax/NN)\n",
      "  de/IN)\n",
      "\n",
      "\n",
      "['j', 'org', 'wunsch', 'ham', 'dl', 'dtl', 'joerg', 'wunsch', 'uriah', 'sax', 'de']\n",
      "N_grams\n",
      "4-gram:  ['j org wunsch ham', 'org wunsch ham dl', 'wunsch ham dl dtl', 'ham dl dtl joerg', 'dl dtl joerg wunsch', 'dtl joerg wunsch uriah', 'joerg wunsch uriah sax', 'wunsch uriah sax de']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['if', 'anything', 'can', 'go', 'wrong', 'or']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['anything', 'go', 'wrong']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['anything', 'go', 'wrong']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['anyth', 'go', 'wrong']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['anyth', 'go', 'wrong']\n",
      "After pos_tags: [('anyth', 'NN'), ('go', 'VBP'), ('wrong', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk anyth/NN) go/VBP (mychunk wrong/JJ))\n",
      "\n",
      "\n",
      "['anyth', 'go', 'wrong']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['o', 'o', 'joerg', 'sax', 'de', 'wutcd', 'hadrian', 'hrz', 'tu', 'chemnitz', 'de']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['joerg', 'sax', 'de', 'wutcd', 'hadrian', 'hrz', 'tu', 'chemnitz', 'de']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['joerg', 'sax', 'de', 'wutcd', 'hadrian', 'hrz', 'tu', 'chemnitz', 'de']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['joerg', 'sax', 'de', 'wutcd', 'hadrian', 'hrz', 'tu', 'chemnitz', 'de']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['joerg', 'sax', 'de', 'wutcd', 'hadrian', 'hrz', 'tu', 'chemnitz', 'de']\n",
      "After pos_tags: [('joerg', 'NN'), ('sax', 'NN'), ('de', 'IN'), ('wutcd', 'FW'), ('hadrian', 'JJ'), ('hrz', 'NN'), ('tu', 'NN'), ('chemnitz', 'NN'), ('de', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk joerg/NN sax/NN)\n",
      "  de/IN\n",
      "  wutcd/FW\n",
      "  (mychunk hadrian/JJ)\n",
      "  (mychunk hrz/NN tu/NN chemnitz/NN)\n",
      "  de/IN)\n",
      "\n",
      "\n",
      "['joerg', 'sax', 'de', 'wutcd', 'hadrian', 'hrz', 'tu', 'chemnitz', 'de']\n",
      "N_grams\n",
      "4-gram:  ['joerg sax de wutcd', 'sax de wutcd hadrian', 'de wutcd hadrian hrz', 'wutcd hadrian hrz tu', 'hadrian hrz tu chemnitz', 'hrz tu chemnitz de']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['it', 'will', 'joerg', 'wunsch', 'tcd', 'dresden', 'de']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['joerg', 'wunsch', 'tcd', 'dresden', 'de']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['joerg', 'wunsch', 'tcd', 'dresden', 'de']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['joerg', 'wunsch', 'tcd', 'dresden', 'de']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['joerg', 'wunsch', 'tcd', 'dresden', 'de']\n",
      "After pos_tags: [('joerg', 'NN'), ('wunsch', 'NN'), ('tcd', 'JJ'), ('dresden', 'NN'), ('de', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk joerg/NN wunsch/NN tcd/JJ) (mychunk dresden/NN de/NN))\n",
      "\n",
      "\n",
      "['joerg', 'wunsch', 'tcd', 'dresden', 'de']\n",
      "N_grams\n",
      "4-gram:  ['joerg wunsch tcd dresden', 'wunsch tcd dresden de']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['newsgroups', 'rec', 'autos']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'rec', 'autos']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'rec', 'auto']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'rec', 'auto']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'rec', 'auto']\n",
      "After pos_tags: [('newsgroup', 'NN'), ('rec', 'VBZ'), ('auto', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk newsgroup/NN) rec/VBZ (mychunk auto/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'rec', 'auto']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'rochester', 'udel', 'gatech', 'howland', 'reston', 'ans', 'net', 'zaphod', 'mps', 'ohio', 'state', 'edu', 'uwm', 'edu', 'linac', 'att', 'cbnewsm', 'smartin']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'rochester', 'udel', 'gatech', 'howland', 'reston', 'ans', 'net', 'zaphod', 'mps', 'ohio', 'state', 'edu', 'uwm', 'edu', 'linac', 'att', 'cbnewsm', 'smartin']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'rochester', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'uwm', 'edu', 'linac', 'att', 'cbnewsm', 'smartin']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rochest', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'uwm', 'edu', 'linac', 'att', 'cbnewsm', 'smartin']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rochest', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'uwm', 'edu', 'linac', 'att', 'cbnewsm', 'smartin']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('rochest', 'VBP'), ('udel', 'JJ'), ('gatech', 'NN'), ('howland', 'NN'), ('reston', 'VBZ'), ('an', 'DT'), ('net', 'JJ'), ('zaphod', 'NN'), ('mp', 'NN'), ('ohio', 'NN'), ('state', 'NN'), ('edu', 'NN'), ('uwm', 'JJ'), ('edu', 'NN'), ('linac', 'NN'), ('att', 'NN'), ('cbnewsm', 'NN'), ('smartin', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN)\n",
      "  rochest/VBP\n",
      "  (mychunk udel/JJ)\n",
      "  (mychunk gatech/NN howland/NN)\n",
      "  reston/VBZ\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk zaphod/NN mp/NN ohio/NN state/NN edu/NN uwm/JJ)\n",
      "  (mychunk edu/NN linac/NN att/NN cbnewsm/NN smartin/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'rochest', 'udel', 'gatech', 'howland', 'reston', 'an', 'net', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'uwm', 'edu', 'linac', 'att', 'cbnewsm', 'smartin']\n",
      "N_grams\n",
      "4-gram:  ['path cantaloup srv c', 'cantaloup srv c cmu', 'srv c cmu edu', 'c cmu edu rochest', 'cmu edu rochest udel', 'edu rochest udel gatech', 'rochest udel gatech howland', 'udel gatech howland reston', 'gatech howland reston an', 'howland reston an net', 'reston an net zaphod', 'an net zaphod mp', 'net zaphod mp ohio', 'zaphod mp ohio state', 'mp ohio state edu', 'ohio state edu uwm', 'state edu uwm edu', 'edu uwm edu linac', 'uwm edu linac att', 'edu linac att cbnewsm', 'linac att cbnewsm smartin']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'smartin', 'cbnewsm', 'cb', 'att', 'com', 'steven', 'c', 'martin']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['smartin', 'cbnewsm', 'cb', 'att', 'com', 'steven', 'c', 'martin']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['smartin', 'cbnewsm', 'cb', 'att', 'com', 'steven', 'c', 'martin']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['smartin', 'cbnewsm', 'cb', 'att', 'com', 'steven', 'c', 'martin']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['smartin', 'cbnewsm', 'cb', 'att', 'com', 'steven', 'c', 'martin']\n",
      "After pos_tags: [('smartin', 'NN'), ('cbnewsm', 'NN'), ('cb', 'NN'), ('att', 'NN'), ('com', 'NN'), ('steven', 'RB'), ('c', 'VBD'), ('martin', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk smartin/NN cbnewsm/NN cb/NN att/NN com/NN)\n",
      "  steven/RB\n",
      "  (mychunk c/VBD)\n",
      "  (mychunk martin/NN))\n",
      "\n",
      "\n",
      "['smartin', 'cbnewsm', 'cb', 'att', 'com', 'steven', 'c', 'martin']\n",
      "N_grams\n",
      "4-gram:  ['smartin cbnewsm cb att', 'cbnewsm cb att com', 'cb att com steven', 'att com steven c', 'com steven c martin']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 're', 'car', 'buying', 'story', 'was', 'christ', 'another', 'dealer', 'service', 'scam']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'car', 'buying', 'story', 'christ', 'another', 'dealer', 'service', 'scam']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'car', 'buying', 'story', 'christ', 'another', 'dealer', 'service', 'scam']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'car', 'buy', 'stori', 'christ', 'anoth', 'dealer', 'servic', 'scam']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'car', 'buy', 'stori', 'christ', 'anoth', 'dealer', 'servic', 'scam']\n",
      "After pos_tags: [('subject', 'JJ'), ('car', 'NN'), ('buy', 'NN'), ('stori', 'VBP'), ('christ', 'JJ'), ('anoth', 'DT'), ('dealer', 'NN'), ('servic', 'NN'), ('scam', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk subject/JJ)\n",
      "  (mychunk car/NN buy/NN)\n",
      "  stori/VBP\n",
      "  (mychunk christ/JJ)\n",
      "  anoth/DT\n",
      "  (mychunk dealer/NN servic/NN scam/NN))\n",
      "\n",
      "\n",
      "['subject', 'car', 'buy', 'stori', 'christ', 'anoth', 'dealer', 'servic', 'scam']\n",
      "N_grams\n",
      "4-gram:  ['subject car buy stori', 'car buy stori christ', 'buy stori christ anoth', 'stori christ anoth dealer', 'christ anoth dealer servic', 'anoth dealer servic scam']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'at', 't']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ']\n",
      "After pos_tags: [('organ', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk organ/NN))\n",
      "\n",
      "\n",
      "['organ']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['distribution', 'usa']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['distribution', 'usa']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['distribution', 'usa']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['distribut', 'usa']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['distribut', 'usa']\n",
      "After pos_tags: [('distribut', 'NN'), ('usa', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk distribut/NN usa/NN))\n",
      "\n",
      "\n",
      "['distribut', 'usa']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'fri', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'fri', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'fri', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'fri', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'fri', 'apr', 'gmt']\n",
      "After pos_tags: [('date', 'NN'), ('fri', 'NN'), ('apr', 'NN'), ('gmt', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN fri/NN apr/NN gmt/NN))\n",
      "\n",
      "\n",
      "['date', 'fri', 'apr', 'gmt']\n",
      "N_grams\n",
      "4-gram:  ['date fri apr gmt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'c', 'lb', 'a', 'jle', 'cbnewsm', 'cb', 'att', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'c', 'lb', 'jle', 'cbnewsm', 'cb', 'att', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'c', 'lb', 'jle', 'cbnewsm', 'cb', 'att', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'c', 'lb', 'jle', 'cbnewsm', 'cb', 'att', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'c', 'lb', 'jle', 'cbnewsm', 'cb', 'att', 'com']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('c', 'NN'), ('lb', 'NN'), ('jle', 'NN'), ('cbnewsm', 'NN'), ('cb', 'NN'), ('att', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk\n",
      "    messag/NN\n",
      "    id/NN\n",
      "    c/NN\n",
      "    lb/NN\n",
      "    jle/NN\n",
      "    cbnewsm/NN\n",
      "    cb/NN\n",
      "    att/NN\n",
      "    com/NN))\n",
      "\n",
      "\n",
      "['messag', 'id', 'c', 'lb', 'jle', 'cbnewsm', 'cb', 'att', 'com']\n",
      "N_grams\n",
      "4-gram:  ['messag id c lb', 'id c lb jle', 'c lb jle cbnewsm', 'lb jle cbnewsm cb', 'jle cbnewsm cb att', 'cbnewsm cb att com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['references', 'apr', 'newsgate', 'sps', 'mot', 'com']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['references', 'apr', 'newsgate', 'sps', 'mot', 'com']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reference', 'apr', 'newsgate', 'sps', 'mot', 'com']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['refer', 'apr', 'newsgat', 'sp', 'mot', 'com']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['refer', 'apr', 'newsgat', 'sp', 'mot', 'com']\n",
      "After pos_tags: [('refer', 'NN'), ('apr', 'NN'), ('newsgat', 'JJ'), ('sp', 'NN'), ('mot', 'NN'), ('com', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk refer/NN apr/NN newsgat/JJ)\n",
      "  (mychunk sp/NN mot/NN com/NN))\n",
      "\n",
      "\n",
      "['refer', 'apr', 'newsgat', 'sp', 'mot', 'com']\n",
      "N_grams\n",
      "4-gram:  ['refer apr newsgat sp', 'apr newsgat sp mot', 'newsgat sp mot com']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'article', 'apr', 'newsgate', 'sps', 'mot', 'com', 'by', 'markm', 'bigfoot', 'sps', 'mot', 'com', 'mark', 'monninger']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['article', 'apr', 'newsgate', 'sps', 'mot', 'com', 'markm', 'bigfoot', 'sps', 'mot', 'com', 'mark', 'monninger']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['article', 'apr', 'newsgate', 'sps', 'mot', 'com', 'markm', 'bigfoot', 'sps', 'mot', 'com', 'mark', 'monninger']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['articl', 'apr', 'newsgat', 'sp', 'mot', 'com', 'markm', 'bigfoot', 'sp', 'mot', 'com', 'mark', 'monning']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['articl', 'apr', 'newsgat', 'sp', 'mot', 'com', 'markm', 'bigfoot', 'sp', 'mot', 'com', 'mark', 'monning']\n",
      "After pos_tags: [('articl', 'NN'), ('apr', 'NN'), ('newsgat', 'JJ'), ('sp', 'NN'), ('mot', 'NN'), ('com', 'NN'), ('markm', 'NN'), ('bigfoot', 'NN'), ('sp', 'NN'), ('mot', 'NN'), ('com', 'NN'), ('mark', 'NN'), ('monning', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk articl/NN apr/NN newsgat/JJ)\n",
      "  (mychunk\n",
      "    sp/NN\n",
      "    mot/NN\n",
      "    com/NN\n",
      "    markm/NN\n",
      "    bigfoot/NN\n",
      "    sp/NN\n",
      "    mot/NN\n",
      "    com/NN\n",
      "    mark/NN\n",
      "    monning/NN))\n",
      "\n",
      "\n",
      "['articl', 'apr', 'newsgat', 'sp', 'mot', 'com', 'markm', 'bigfoot', 'sp', 'mot', 'com', 'mark', 'monning']\n",
      "N_grams\n",
      "4-gram:  ['articl apr newsgat sp', 'apr newsgat sp mot', 'newsgat sp mot com', 'sp mot com markm', 'mot com markm bigfoot', 'com markm bigfoot sp', 'markm bigfoot sp mot', 'bigfoot sp mot com', 'sp mot com mark', 'mot com mark monning']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['this', 'kind', 'of', 'behavior', 'is', 'what', 'i', 'was', 'shocked', 'by', 'in', 'my', 'experience', 'for']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['kind', 'behavior', 'shocked', 'experience']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['kind', 'behavior', 'shocked', 'experience']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['kind', 'behavior', 'shock', 'experi']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['kind', 'behavior', 'shock', 'experi']\n",
      "After pos_tags: [('kind', 'NN'), ('behavior', 'NN'), ('shock', 'NN'), ('experi', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk kind/NN behavior/NN shock/NN experi/NN))\n",
      "\n",
      "\n",
      "['kind', 'behavior', 'shock', 'experi']\n",
      "N_grams\n",
      "4-gram:  ['kind behavior shock experi']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['crying', 'out', 'loud', 'how', 'do', 'these', 'turkeys', 'think', 'they', 'can', 'talk', 'to', 'customers']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['crying', 'loud', 'turkeys', 'think', 'talk', 'customers']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['cry', 'loud', 'turkey', 'think', 'talk', 'customer']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['cri', 'loud', 'turkey', 'think', 'talk', 'custom']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['cri', 'loud', 'turkey', 'think', 'talk', 'custom']\n",
      "After pos_tags: [('cri', 'NN'), ('loud', 'JJ'), ('turkey', 'NN'), ('think', 'VBP'), ('talk', 'NN'), ('custom', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk cri/NN loud/JJ)\n",
      "  (mychunk turkey/NN)\n",
      "  think/VBP\n",
      "  (mychunk talk/NN custom/NN))\n",
      "\n",
      "\n",
      "['cri', 'loud', 'turkey', 'think', 'talk', 'custom']\n",
      "N_grams\n",
      "4-gram:  ['cri loud turkey think', 'loud turkey think talk', 'turkey think talk custom']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['this', 'way', 'and', 'still', 'stay', 'in', 'business', 'again', 'i', 'don', 't', 'expect', 'sales', 'people', 'to']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['way', 'still', 'stay', 'business', 'expect', 'sales', 'people']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['way', 'still', 'stay', 'business', 'expect', 'sale', 'people']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['way', 'still', 'stay', 'busi', 'expect', 'sale', 'peopl']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['way', 'still', 'stay', 'busi', 'expect', 'sale', 'peopl']\n",
      "After pos_tags: [('way', 'NN'), ('still', 'RB'), ('stay', 'VB'), ('busi', 'RB'), ('expect', 'JJ'), ('sale', 'NN'), ('peopl', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk way/NN)\n",
      "  still/RB\n",
      "  stay/VB\n",
      "  busi/RB\n",
      "  (mychunk expect/JJ)\n",
      "  (mychunk sale/NN peopl/NN))\n",
      "\n",
      "\n",
      "['way', 'still', 'stay', 'busi', 'expect', 'sale', 'peopl']\n",
      "N_grams\n",
      "4-gram:  ['way still stay busi', 'still stay busi expect', 'stay busi expect sale', 'busi expect sale peopl']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['bow', 'scrape', 'and', 'grovel', 'in', 'my', 'presence', 'but', 'i', 'sure', 'don', 't', 'expect', 'to', 'be']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['bow', 'scrape', 'grovel', 'presence', 'sure', 'expect']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['bow', 'scrape', 'grovel', 'presence', 'sure', 'expect']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['bow', 'scrape', 'grovel', 'presenc', 'sure', 'expect']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['bow', 'scrape', 'grovel', 'presenc', 'sure', 'expect']\n",
      "After pos_tags: [('bow', 'NN'), ('scrape', 'NN'), ('grovel', 'NN'), ('presenc', 'NN'), ('sure', 'JJ'), ('expect', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk bow/NN scrape/NN grovel/NN presenc/NN sure/JJ)\n",
      "  expect/VBP)\n",
      "\n",
      "\n",
      "['bow', 'scrape', 'grovel', 'presenc', 'sure', 'expect']\n",
      "N_grams\n",
      "4-gram:  ['bow scrape grovel presenc', 'scrape grovel presenc sure', 'grovel presenc sure expect']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['abused', 'either', 'i', 'was', 'very', 'surprised', 'by', 'the', 'way', 'the', 'sales', 'people', 'talked', 'to']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['abused', 'either', 'surprised', 'way', 'sales', 'people', 'talked']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['abused', 'either', 'surprised', 'way', 'sale', 'people', 'talked']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['abus', 'either', 'surpris', 'way', 'sale', 'peopl', 'talk']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['abus', 'either', 'surpris', 'way', 'sale', 'peopl', 'talk']\n",
      "After pos_tags: [('abus', 'NN'), ('either', 'DT'), ('surpris', 'JJ'), ('way', 'NN'), ('sale', 'NN'), ('peopl', 'NN'), ('talk', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk abus/NN)\n",
      "  either/DT\n",
      "  (mychunk surpris/JJ)\n",
      "  (mychunk way/NN sale/NN peopl/NN talk/NN))\n",
      "\n",
      "\n",
      "['abus', 'either', 'surpris', 'way', 'sale', 'peopl', 'talk']\n",
      "N_grams\n",
      "4-gram:  ['abus either surpris way', 'either surpris way sale', 'surpris way sale peopl', 'way sale peopl talk']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['me', 'and', 'in', 'other', 'negotiating', 'sessions', 'i', 'overheard', 'in', 'neighboring', 'sales']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['negotiating', 'sessions', 'overheard', 'neighboring', 'sales']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['negotiating', 'session', 'overheard', 'neighboring', 'sale']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['negoti', 'session', 'overheard', 'neighbor', 'sale']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['negoti', 'session', 'overheard', 'neighbor', 'sale']\n",
      "After pos_tags: [('negoti', 'JJ'), ('session', 'NN'), ('overheard', 'RB'), ('neighbor', 'JJ'), ('sale', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk negoti/JJ)\n",
      "  (mychunk session/NN)\n",
      "  overheard/RB\n",
      "  (mychunk neighbor/JJ)\n",
      "  (mychunk sale/NN))\n",
      "\n",
      "\n",
      "['negoti', 'session', 'overheard', 'neighbor', 'sale']\n",
      "N_grams\n",
      "4-gram:  ['negoti session overheard neighbor', 'session overheard neighbor sale']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['cubicles', 'evidently', 'their', 'success', 'rate', 'is', 'high', 'enough', 'that', 'they', 'continue']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['cubicles', 'evidently', 'success', 'rate', 'high', 'enough', 'continue']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['cubicle', 'evidently', 'success', 'rate', 'high', 'enough', 'continue']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['cubicl', 'evid', 'success', 'rate', 'high', 'enough', 'continu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['cubicl', 'evid', 'success', 'rate', 'high', 'enough', 'continu']\n",
      "After pos_tags: [('cubicl', 'NN'), ('evid', 'JJ'), ('success', 'NN'), ('rate', 'NN'), ('high', 'JJ'), ('enough', 'RB'), ('continu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk cubicl/NN evid/JJ)\n",
      "  (mychunk success/NN rate/NN high/JJ)\n",
      "  enough/RB\n",
      "  (mychunk continu/NN))\n",
      "\n",
      "\n",
      "['cubicl', 'evid', 'success', 'rate', 'high', 'enough', 'continu']\n",
      "N_grams\n",
      "4-gram:  ['cubicl evid success rate', 'evid success rate high', 'success rate high enough', 'rate high enough continu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['to', 'do', 'business', 'this', 'way', 'there', 'must', 'be', 'a', 'lot', 'of', 'people', 'out', 'there', 'who', 'are']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['business', 'way', 'must', 'lot', 'people']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['business', 'way', 'must', 'lot', 'people']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['busi', 'way', 'must', 'lot', 'peopl']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['busi', 'way', 'must', 'lot', 'peopl']\n",
      "After pos_tags: [('busi', 'JJ'), ('way', 'NN'), ('must', 'MD'), ('lot', 'VB'), ('peopl', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk busi/JJ)\n",
      "  (mychunk way/NN)\n",
      "  must/MD\n",
      "  lot/VB\n",
      "  (mychunk peopl/NN))\n",
      "\n",
      "\n",
      "['busi', 'way', 'must', 'lot', 'peopl']\n",
      "N_grams\n",
      "4-gram:  ['busi way must lot', 'way must lot peopl']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['easy', 'to', 'intimidate']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['easy', 'intimidate']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['easy', 'intimidate']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['easi', 'intimid']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['easi', 'intimid']\n",
      "After pos_tags: [('easi', 'NN'), ('intimid', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk easi/NN intimid/NN))\n",
      "\n",
      "\n",
      "['easi', 'intimid']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['a', 'couple', 'of', 'months', 'ago', 'i', 'went', 'to', 'a', 'dealership', 'to', 'test', 'drive', 'a', 'car', 'afterwards']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['couple', 'months', 'ago', 'went', 'dealership', 'test', 'drive', 'car', 'afterwards']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['couple', 'month', 'ago', 'went', 'dealership', 'test', 'drive', 'car', 'afterwards']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['coupl', 'month', 'ago', 'went', 'dealership', 'test', 'drive', 'car', 'afterward']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['coupl', 'month', 'ago', 'went', 'dealership', 'test', 'drive', 'car', 'afterward']\n",
      "After pos_tags: [('coupl', 'JJ'), ('month', 'NN'), ('ago', 'RB'), ('went', 'VBD'), ('dealership', 'JJ'), ('test', 'NN'), ('drive', 'NN'), ('car', 'NN'), ('afterward', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk coupl/JJ)\n",
      "  (mychunk month/NN)\n",
      "  ago/RB\n",
      "  (mychunk went/VBD dealership/JJ)\n",
      "  (mychunk test/NN drive/NN car/NN afterward/NN))\n",
      "\n",
      "\n",
      "['coupl', 'month', 'ago', 'went', 'dealership', 'test', 'drive', 'car', 'afterward']\n",
      "N_grams\n",
      "4-gram:  ['coupl month ago went', 'month ago went dealership', 'ago went dealership test', 'went dealership test drive', 'dealership test drive car', 'test drive car afterward']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['we', 'sat', 'down', 'to', 'discuss', 'prices', 'i', 'explained', 'that', 'i', 'wanted', 'a', 'car', 'just', 'like', 'the']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['sat', 'discuss', 'prices', 'explained', 'wanted', 'car', 'like']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['sat', 'discus', 'price', 'explained', 'wanted', 'car', 'like']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['sat', 'discu', 'price', 'explain', 'want', 'car', 'like']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['sat', 'discu', 'price', 'explain', 'want', 'car', 'like']\n",
      "After pos_tags: [('sat', 'JJ'), ('discu', 'NN'), ('price', 'NN'), ('explain', 'NN'), ('want', 'VBP'), ('car', 'NN'), ('like', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk sat/JJ)\n",
      "  (mychunk discu/NN price/NN explain/NN)\n",
      "  want/VBP\n",
      "  (mychunk car/NN)\n",
      "  like/IN)\n",
      "\n",
      "\n",
      "['sat', 'discu', 'price', 'explain', 'want', 'car', 'like']\n",
      "N_grams\n",
      "4-gram:  ['sat discu price explain', 'discu price explain want', 'price explain want car', 'explain want car like']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['one', 'i', 'drove', 'but', 'in', 'a', 'different', 'color', 'he', 'said', 'he', 'could', 'get', 'one', 'exactly', 'like']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['one', 'drove', 'different', 'color', 'said', 'could', 'get', 'one', 'exactly', 'like']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['one', 'drove', 'different', 'color', 'said', 'could', 'get', 'one', 'exactly', 'like']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['one', 'drove', 'differ', 'color', 'said', 'could', 'get', 'one', 'exactli', 'like']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['one', 'drove', 'differ', 'color', 'said', 'could', 'get', 'one', 'exactli', 'like']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pos_tags: [('one', 'CD'), ('drove', 'NN'), ('differ', 'NN'), ('color', 'NN'), ('said', 'VBD'), ('could', 'MD'), ('get', 'VB'), ('one', 'CD'), ('exactli', 'NN'), ('like', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  one/CD\n",
      "  (mychunk drove/NN differ/NN color/NN said/VBD)\n",
      "  could/MD\n",
      "  get/VB\n",
      "  one/CD\n",
      "  (mychunk exactli/NN)\n",
      "  like/IN)\n",
      "\n",
      "\n",
      "['one', 'drove', 'differ', 'color', 'said', 'could', 'get', 'one', 'exactli', 'like']\n",
      "N_grams\n",
      "4-gram:  ['one drove differ color', 'drove differ color said', 'differ color said could', 'color said could get', 'said could get one', 'could get one exactli', 'get one exactli like']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['i', 'wanted', 'from', 'the', 'dealer', 'network', 'within', 'a', 'day', 'we', 'then', 'negotiated', 'a', 'price', 'and']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['wanted', 'dealer', 'network', 'within', 'day', 'negotiated', 'price']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['wanted', 'dealer', 'network', 'within', 'day', 'negotiated', 'price']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['want', 'dealer', 'network', 'within', 'day', 'negoti', 'price']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['want', 'dealer', 'network', 'within', 'day', 'negoti', 'price']\n",
      "After pos_tags: [('want', 'NN'), ('dealer', 'NN'), ('network', 'NN'), ('within', 'IN'), ('day', 'NN'), ('negoti', 'JJ'), ('price', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk want/NN dealer/NN network/NN)\n",
      "  within/IN\n",
      "  (mychunk day/NN negoti/JJ)\n",
      "  (mychunk price/NN))\n",
      "\n",
      "\n",
      "['want', 'dealer', 'network', 'within', 'day', 'negoti', 'price']\n",
      "N_grams\n",
      "4-gram:  ['want dealer network within', 'dealer network within day', 'network within day negoti', 'within day negoti price']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['signed', 'the', 'deal']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['signed', 'deal']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['signed', 'deal']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['sign', 'deal']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['sign', 'deal']\n",
      "After pos_tags: [('sign', 'NN'), ('deal', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk sign/NN deal/NN))\n",
      "\n",
      "\n",
      "['sign', 'deal']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['next', 'day', 'i', 'get', 'a', 'call', 'he', 'explains', 'that', 'they', 'goofed', 'and', 'they', 'had', 'neglected']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['next', 'day', 'get', 'call', 'explains', 'goofed', 'neglected']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['next', 'day', 'get', 'call', 'explains', 'goofed', 'neglected']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['next', 'day', 'get', 'call', 'explain', 'goof', 'neglect']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['next', 'day', 'get', 'call', 'explain', 'goof', 'neglect']\n",
      "After pos_tags: [('next', 'JJ'), ('day', 'NN'), ('get', 'VB'), ('call', 'JJ'), ('explain', 'NN'), ('goof', 'NN'), ('neglect', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk next/JJ)\n",
      "  (mychunk day/NN)\n",
      "  get/VB\n",
      "  (mychunk call/JJ)\n",
      "  (mychunk explain/NN goof/NN neglect/NN))\n",
      "\n",
      "\n",
      "['next', 'day', 'get', 'call', 'explain', 'goof', 'neglect']\n",
      "N_grams\n",
      "4-gram:  ['next day get call', 'day get call explain', 'get call explain goof', 'call explain goof neglect']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['to', 'take', 'into', 'account', 'a', 'price', 'increase', 'the', 'last', 'price', 'increase', 'had', 'occurred']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['take', 'account', 'price', 'increase', 'last', 'price', 'increase', 'occurred']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['take', 'account', 'price', 'increase', 'last', 'price', 'increase', 'occurred']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['take', 'account', 'price', 'increas', 'last', 'price', 'increas', 'occur']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['take', 'account', 'price', 'increas', 'last', 'price', 'increas', 'occur']\n",
      "After pos_tags: [('take', 'VB'), ('account', 'NN'), ('price', 'NN'), ('increas', 'NNS'), ('last', 'JJ'), ('price', 'NN'), ('increas', 'NNS'), ('occur', 'VBP')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  take/VB\n",
      "  (mychunk account/NN price/NN increas/NNS last/JJ)\n",
      "  (mychunk price/NN increas/NNS)\n",
      "  occur/VBP)\n",
      "\n",
      "\n",
      "['take', 'account', 'price', 'increas', 'last', 'price', 'increas', 'occur']\n",
      "N_grams\n",
      "4-gram:  ['take account price increas', 'account price increas last', 'price increas last price', 'increas last price increas', 'last price increas occur']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['over', 'months', 'prior', 'to', 'my', 'visit', 'if', 'i', 'still', 'wanted', 'the', 'car', 'i', 'would']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['months', 'prior', 'visit', 'still', 'wanted', 'car', 'would']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['month', 'prior', 'visit', 'still', 'wanted', 'car', 'would']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['month', 'prior', 'visit', 'still', 'want', 'car', 'would']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['month', 'prior', 'visit', 'still', 'want', 'car', 'would']\n",
      "After pos_tags: [('month', 'NN'), ('prior', 'RB'), ('visit', 'NN'), ('still', 'RB'), ('want', 'JJ'), ('car', 'NN'), ('would', 'MD')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk month/NN)\n",
      "  prior/RB\n",
      "  (mychunk visit/NN)\n",
      "  still/RB\n",
      "  (mychunk want/JJ)\n",
      "  (mychunk car/NN)\n",
      "  would/MD)\n",
      "\n",
      "\n",
      "['month', 'prior', 'visit', 'still', 'want', 'car', 'would']\n",
      "N_grams\n",
      "4-gram:  ['month prior visit still', 'prior visit still want', 'visit still want car', 'still want car would']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['have', 'to', 'fork', 'over', 'another', 'as', 'an', 'alternative', 'they', 'would', 'honor', 'the']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['fork', 'another', 'alternative', 'would', 'honor']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['fork', 'another', 'alternative', 'would', 'honor']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['fork', 'anoth', 'altern', 'would', 'honor']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['fork', 'anoth', 'altern', 'would', 'honor']\n",
      "After pos_tags: [('fork', 'NN'), ('anoth', 'DT'), ('altern', 'JJ'), ('would', 'MD'), ('honor', 'VB')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk fork/NN) anoth/DT (mychunk altern/JJ) would/MD honor/VB)\n",
      "\n",
      "\n",
      "['fork', 'anoth', 'altern', 'would', 'honor']\n",
      "N_grams\n",
      "4-gram:  ['fork anoth altern would', 'anoth altern would honor']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['price', 'if', 'i', 'bought', 'the', 'car', 'i', 'test', 'drove', 'which', 'had', 'been', 'sitting', 'around', 'for']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['price', 'bought', 'car', 'test', 'drove', 'sitting', 'around']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['price', 'bought', 'car', 'test', 'drove', 'sitting', 'around']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['price', 'bought', 'car', 'test', 'drove', 'sit', 'around']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['price', 'bought', 'car', 'test', 'drove', 'sit', 'around']\n",
      "After pos_tags: [('price', 'NN'), ('bought', 'VBD'), ('car', 'NN'), ('test', 'NN'), ('drove', 'VBD'), ('sit', 'NN'), ('around', 'IN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk price/NN bought/VBD)\n",
      "  (mychunk car/NN test/NN drove/VBD)\n",
      "  (mychunk sit/NN)\n",
      "  around/IN)\n",
      "\n",
      "\n",
      "['price', 'bought', 'car', 'test', 'drove', 'sit', 'around']\n",
      "N_grams\n",
      "4-gram:  ['price bought car test', 'bought car test drove', 'car test drove sit', 'test drove sit around']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['months', 'and', 'had', 'a', 'few', 'miles', 'on', 'it', 'i', 'said', 'goodbye', 'this', 'was', 'a', 'good']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['months', 'miles', 'said', 'goodbye', 'good']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['month', 'mile', 'said', 'goodbye', 'good']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['month', 'mile', 'said', 'goodby', 'good']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['month', 'mile', 'said', 'goodby', 'good']\n",
      "After pos_tags: [('month', 'NN'), ('mile', 'NN'), ('said', 'VBD'), ('goodby', 'JJ'), ('good', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk month/NN mile/NN said/VBD goodby/JJ good/JJ))\n",
      "\n",
      "\n",
      "['month', 'mile', 'said', 'goodby', 'good']\n",
      "N_grams\n",
      "4-gram:  ['month mile said goodby', 'mile said goodby good']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['example', 'of', 'how', 'they', 'can', 'lowball', 'you', 'and', 'still', 'cover', 'their', 'butts', 'it', 's', 'too']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['example', 'lowball', 'still', 'cover', 'butts']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['example', 'lowball', 'still', 'cover', 'butt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['exampl', 'lowbal', 'still', 'cover', 'butt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['exampl', 'lowbal', 'still', 'cover', 'butt']\n",
      "After pos_tags: [('exampl', 'NN'), ('lowbal', 'NN'), ('still', 'RB'), ('cover', 'VBZ'), ('butt', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk exampl/NN lowbal/NN)\n",
      "  still/RB\n",
      "  cover/VBZ\n",
      "  (mychunk butt/NN))\n",
      "\n",
      "\n",
      "['exampl', 'lowbal', 'still', 'cover', 'butt']\n",
      "N_grams\n",
      "4-gram:  ['exampl lowbal still cover', 'lowbal still cover butt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['bad', 'more', 'people', 'don', 't', 'demand', 'honesty', 'or', 'these', 'types', 'of', 'dealers', 'would']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['bad', 'people', 'demand', 'honesty', 'types', 'dealers', 'would']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['bad', 'people', 'demand', 'honesty', 'type', 'dealer', 'would']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['bad', 'peopl', 'demand', 'honesti', 'type', 'dealer', 'would']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['bad', 'peopl', 'demand', 'honesti', 'type', 'dealer', 'would']\n",
      "After pos_tags: [('bad', 'JJ'), ('peopl', 'JJ'), ('demand', 'NN'), ('honesti', 'NN'), ('type', 'NN'), ('dealer', 'NN'), ('would', 'MD')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk bad/JJ peopl/JJ)\n",
      "  (mychunk demand/NN honesti/NN type/NN dealer/NN)\n",
      "  would/MD)\n",
      "\n",
      "\n",
      "['bad', 'peopl', 'demand', 'honesti', 'type', 'dealer', 'would']\n",
      "N_grams\n",
      "4-gram:  ['bad peopl demand honesti', 'peopl demand honesti type', 'demand honesti type dealer', 'honesti type dealer would']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['no', 'longer', 'be', 'in', 'business']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['longer', 'business']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['longer', 'business']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['longer', 'busi']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['longer', 'busi']\n",
      "After pos_tags: [('longer', 'JJR'), ('busi', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk longer/JJR) (mychunk busi/NN))\n",
      "\n",
      "\n",
      "['longer', 'busi']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['the', 'next', 'dealership', 'i', 'went', 'to', 'was', 'straightforward', 'and', 'honest', 'first', 'thing', 'the']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['next', 'dealership', 'went', 'straightforward', 'honest', 'first', 'thing']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['next', 'dealership', 'went', 'straightforward', 'honest', 'first', 'thing']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['next', 'dealership', 'went', 'straightforward', 'honest', 'first', 'thing']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['next', 'dealership', 'went', 'straightforward', 'honest', 'first', 'thing']\n",
      "After pos_tags: [('next', 'JJ'), ('dealership', 'NN'), ('went', 'VBD'), ('straightforward', 'RB'), ('honest', 'JJS'), ('first', 'JJ'), ('thing', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk next/JJ)\n",
      "  (mychunk dealership/NN went/VBD)\n",
      "  straightforward/RB\n",
      "  (mychunk honest/JJS first/JJ)\n",
      "  (mychunk thing/NN))\n",
      "\n",
      "\n",
      "['next', 'dealership', 'went', 'straightforward', 'honest', 'first', 'thing']\n",
      "N_grams\n",
      "4-gram:  ['next dealership went straightforward', 'dealership went straightforward honest', 'went straightforward honest first', 'straightforward honest first thing']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['salesman', 'said', 'was', 'lets', 's', 'see', 'what', 'you', 'have', 'for', 'dealer', 'cost', 'and', 'work', 'out']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['salesman', 'said', 'lets', 'see', 'dealer', 'cost', 'work']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['salesman', 'said', 'let', 'see', 'dealer', 'cost', 'work']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['salesman', 'said', 'let', 'see', 'dealer', 'cost', 'work']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['salesman', 'said', 'let', 'see', 'dealer', 'cost', 'work']\n",
      "After pos_tags: [('salesman', 'NN'), ('said', 'VBD'), ('let', 'JJ'), ('see', 'VB'), ('dealer', 'VB'), ('cost', 'NN'), ('work', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk salesman/NN said/VBD let/JJ)\n",
      "  see/VB\n",
      "  dealer/VB\n",
      "  (mychunk cost/NN work/NN))\n",
      "\n",
      "\n",
      "['salesman', 'said', 'let', 'see', 'dealer', 'cost', 'work']\n",
      "N_grams\n",
      "4-gram:  ['salesman said let see', 'said let see dealer', 'let see dealer cost', 'see dealer cost work']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['how', 'much', 'profit', 'i', 'should', 'make', 'the', 'deal', 'went', 'through', 'with', 'no', 'problems']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['much', 'profit', 'make', 'deal', 'went', 'problems']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['much', 'profit', 'make', 'deal', 'went', 'problem']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['much', 'profit', 'make', 'deal', 'went', 'problem']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['much', 'profit', 'make', 'deal', 'went', 'problem']\n",
      "After pos_tags: [('much', 'JJ'), ('profit', 'NN'), ('make', 'VBP'), ('deal', 'NN'), ('went', 'VBD'), ('problem', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk much/JJ)\n",
      "  (mychunk profit/NN)\n",
      "  make/VBP\n",
      "  (mychunk deal/NN went/VBD)\n",
      "  (mychunk problem/NN))\n",
      "\n",
      "\n",
      "['much', 'profit', 'make', 'deal', 'went', 'problem']\n",
      "N_grams\n",
      "4-gram:  ['much profit make deal', 'profit make deal went', 'make deal went problem']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['steve']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['steve']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['steve']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['steve']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['steve']\n",
      "After pos_tags: [('steve', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk steve/NN))\n",
      "\n",
      "\n",
      "['steve']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['newsgroups', 'sci', 'space']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['newsgroups', 'sci', 'space']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['newsgroups', 'sci', 'space']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['newsgroup', 'sci', 'space']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['newsgroup', 'sci', 'space']\n",
      "After pos_tags: [('newsgroup', 'JJ'), ('sci', 'VBD'), ('space', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk newsgroup/JJ) (mychunk sci/VBD) (mychunk space/NN))\n",
      "\n",
      "\n",
      "['newsgroup', 'sci', 'space']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'magnesium', 'club', 'cc', 'cmu', 'edu', 'news', 'sei', 'cmu', 'edu', 'cis', 'ohio', 'state', 'edu', 'zaphod', 'mps', 'ohio', 'state', 'edu', 'malgudi', 'oar', 'net', 'news', 'ans', 'net', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'ans', 'net', 'usc', 'cs', 'utexas', 'edu', 'utnut', 'utzoo', 'henry']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['path', 'cantaloupe', 'srv', 'cs', 'cmu', 'edu', 'magnesium', 'club', 'cc', 'cmu', 'edu', 'news', 'sei', 'cmu', 'edu', 'cis', 'ohio', 'state', 'edu', 'zaphod', 'mps', 'ohio', 'state', 'edu', 'malgudi', 'oar', 'net', 'news', 'ans', 'net', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'ans', 'net', 'usc', 'cs', 'utexas', 'edu', 'utnut', 'utzoo', 'henry']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['path', 'cantaloupe', 'srv', 'c', 'cmu', 'edu', 'magnesium', 'club', 'cc', 'cmu', 'edu', 'news', 'sei', 'cmu', 'edu', 'ci', 'ohio', 'state', 'edu', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'malgudi', 'oar', 'net', 'news', 'an', 'net', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'usc', 'c', 'utexas', 'edu', 'utnut', 'utzoo', 'henry']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'magnesium', 'club', 'cc', 'cmu', 'edu', 'news', 'sei', 'cmu', 'edu', 'ci', 'ohio', 'state', 'edu', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'malgudi', 'oar', 'net', 'news', 'an', 'net', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'usc', 'c', 'utexa', 'edu', 'utnut', 'utzoo', 'henri']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'magnesium', 'club', 'cc', 'cmu', 'edu', 'news', 'sei', 'cmu', 'edu', 'ci', 'ohio', 'state', 'edu', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'malgudi', 'oar', 'net', 'news', 'an', 'net', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'usc', 'c', 'utexa', 'edu', 'utnut', 'utzoo', 'henri']\n",
      "After pos_tags: [('path', 'NN'), ('cantaloup', 'NN'), ('srv', 'VBD'), ('c', 'JJ'), ('cmu', 'NN'), ('edu', 'NN'), ('magnesium', 'NN'), ('club', 'NN'), ('cc', 'NN'), ('cmu', 'NN'), ('edu', 'JJ'), ('news', 'NN'), ('sei', 'NN'), ('cmu', 'NN'), ('edu', 'NN'), ('ci', 'NN'), ('ohio', 'NN'), ('state', 'NN'), ('edu', 'NN'), ('zaphod', 'NN'), ('mp', 'NN'), ('ohio', 'NN'), ('state', 'NN'), ('edu', 'NN'), ('malgudi', 'NN'), ('oar', 'FW'), ('net', 'JJ'), ('news', 'NN'), ('an', 'DT'), ('net', 'JJ'), ('europa', 'NN'), ('eng', 'NN'), ('gtefsd', 'NN'), ('com', 'NN'), ('howland', 'NN'), ('reston', 'VBZ'), ('an', 'DT'), ('net', 'JJ'), ('usc', 'NN'), ('c', 'NN'), ('utexa', 'JJ'), ('edu', 'NN'), ('utnut', 'JJ'), ('utzoo', 'JJ'), ('henri', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk path/NN cantaloup/NN srv/VBD c/JJ)\n",
      "  (mychunk cmu/NN edu/NN magnesium/NN club/NN cc/NN cmu/NN edu/JJ)\n",
      "  (mychunk\n",
      "    news/NN\n",
      "    sei/NN\n",
      "    cmu/NN\n",
      "    edu/NN\n",
      "    ci/NN\n",
      "    ohio/NN\n",
      "    state/NN\n",
      "    edu/NN\n",
      "    zaphod/NN\n",
      "    mp/NN\n",
      "    ohio/NN\n",
      "    state/NN\n",
      "    edu/NN\n",
      "    malgudi/NN)\n",
      "  oar/FW\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk news/NN)\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk europa/NN eng/NN gtefsd/NN com/NN howland/NN)\n",
      "  reston/VBZ\n",
      "  an/DT\n",
      "  (mychunk net/JJ)\n",
      "  (mychunk usc/NN c/NN utexa/JJ)\n",
      "  (mychunk edu/NN utnut/JJ utzoo/JJ)\n",
      "  (mychunk henri/NN))\n",
      "\n",
      "\n",
      "['path', 'cantaloup', 'srv', 'c', 'cmu', 'edu', 'magnesium', 'club', 'cc', 'cmu', 'edu', 'news', 'sei', 'cmu', 'edu', 'ci', 'ohio', 'state', 'edu', 'zaphod', 'mp', 'ohio', 'state', 'edu', 'malgudi', 'oar', 'net', 'news', 'an', 'net', 'europa', 'eng', 'gtefsd', 'com', 'howland', 'reston', 'an', 'net', 'usc', 'c', 'utexa', 'edu', 'utnut', 'utzoo', 'henri']\n",
      "N_grams\n",
      "4-gram:  ['path cantaloup srv c', 'cantaloup srv c cmu', 'srv c cmu edu', 'c cmu edu magnesium', 'cmu edu magnesium club', 'edu magnesium club cc', 'magnesium club cc cmu', 'club cc cmu edu', 'cc cmu edu news', 'cmu edu news sei', 'edu news sei cmu', 'news sei cmu edu', 'sei cmu edu ci', 'cmu edu ci ohio', 'edu ci ohio state', 'ci ohio state edu', 'ohio state edu zaphod', 'state edu zaphod mp', 'edu zaphod mp ohio', 'zaphod mp ohio state', 'mp ohio state edu', 'ohio state edu malgudi', 'state edu malgudi oar', 'edu malgudi oar net', 'malgudi oar net news', 'oar net news an', 'net news an net', 'news an net europa', 'an net europa eng', 'net europa eng gtefsd', 'europa eng gtefsd com', 'eng gtefsd com howland', 'gtefsd com howland reston', 'com howland reston an', 'howland reston an net', 'reston an net usc', 'an net usc c', 'net usc c utexa', 'usc c utexa edu', 'c utexa edu utnut', 'utexa edu utnut utzoo', 'edu utnut utzoo henri']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['from', 'henry', 'zoo', 'toronto', 'edu', 'henry', 'spencer']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['henry', 'zoo', 'toronto', 'edu', 'henry', 'spencer']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['henry', 'zoo', 'toronto', 'edu', 'henry', 'spencer']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['henri', 'zoo', 'toronto', 'edu', 'henri', 'spencer']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['henri', 'zoo', 'toronto', 'edu', 'henri', 'spencer']\n",
      "After pos_tags: [('henri', 'NN'), ('zoo', 'NN'), ('toronto', 'IN'), ('edu', 'NN'), ('henri', 'NN'), ('spencer', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk henri/NN zoo/NN)\n",
      "  toronto/IN\n",
      "  (mychunk edu/NN henri/NN spencer/NN))\n",
      "\n",
      "\n",
      "['henri', 'zoo', 'toronto', 'edu', 'henri', 'spencer']\n",
      "N_grams\n",
      "4-gram:  ['henri zoo toronto edu', 'zoo toronto edu henri', 'toronto edu henri spencer']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['subject', 're', 'space', 'station', 'redesign', 'jsc', 'alternative']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['subject', 'space', 'station', 'redesign', 'jsc', 'alternative']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['subject', 'space', 'station', 'redesign', 'jsc', 'alternative']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['subject', 'space', 'station', 'redesign', 'jsc', 'altern']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['subject', 'space', 'station', 'redesign', 'jsc', 'altern']\n",
      "After pos_tags: [('subject', 'JJ'), ('space', 'NN'), ('station', 'NN'), ('redesign', 'NN'), ('jsc', 'NN'), ('altern', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk subject/JJ)\n",
      "  (mychunk space/NN station/NN redesign/NN jsc/NN altern/NN))\n",
      "\n",
      "\n",
      "['subject', 'space', 'station', 'redesign', 'jsc', 'altern']\n",
      "N_grams\n",
      "4-gram:  ['subject space station redesign', 'space station redesign jsc', 'station redesign jsc altern']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['message', 'id', 'c', 'h', 'o', 'zoo', 'toronto', 'edu']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['message', 'id', 'c', 'h', 'zoo', 'toronto', 'edu']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['message', 'id', 'c', 'h', 'zoo', 'toronto', 'edu']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['messag', 'id', 'c', 'h', 'zoo', 'toronto', 'edu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['messag', 'id', 'c', 'h', 'zoo', 'toronto', 'edu']\n",
      "After pos_tags: [('messag', 'NN'), ('id', 'NN'), ('c', 'NN'), ('h', 'NN'), ('zoo', 'NN'), ('toronto', 'NN'), ('edu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk messag/NN id/NN c/NN h/NN zoo/NN toronto/NN edu/NN))\n",
      "\n",
      "\n",
      "['messag', 'id', 'c', 'h', 'zoo', 'toronto', 'edu']\n",
      "N_grams\n",
      "4-gram:  ['messag id c h', 'id c h zoo', 'c h zoo toronto', 'h zoo toronto edu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['date', 'sat', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['date', 'sat', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['date', 'sat', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['date', 'sat', 'apr', 'gmt']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['date', 'sat', 'apr', 'gmt']\n",
      "After pos_tags: [('date', 'NN'), ('sat', 'VBD'), ('apr', 'JJ'), ('gmt', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk date/NN sat/VBD apr/JJ) (mychunk gmt/NN))\n",
      "\n",
      "\n",
      "['date', 'sat', 'apr', 'gmt']\n",
      "N_grams\n",
      "4-gram:  ['date sat apr gmt']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['references', 'apr', 'aio', 'jsc', 'nasa', 'gov', 'apr', 'tm', 'lerc', 'nasa', 'gov', 'ralibinnc', 'f', 'cbl', 'umd', 'edu']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['references', 'apr', 'aio', 'jsc', 'nasa', 'gov', 'apr', 'tm', 'lerc', 'nasa', 'gov', 'ralibinnc', 'f', 'cbl', 'umd', 'edu']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['reference', 'apr', 'aio', 'jsc', 'nasa', 'gov', 'apr', 'tm', 'lerc', 'nasa', 'gov', 'ralibinnc', 'f', 'cbl', 'umd', 'edu']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['refer', 'apr', 'aio', 'jsc', 'nasa', 'gov', 'apr', 'tm', 'lerc', 'nasa', 'gov', 'ralibinnc', 'f', 'cbl', 'umd', 'edu']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['refer', 'apr', 'aio', 'jsc', 'nasa', 'gov', 'apr', 'tm', 'lerc', 'nasa', 'gov', 'ralibinnc', 'f', 'cbl', 'umd', 'edu']\n",
      "After pos_tags: [('refer', 'NN'), ('apr', 'NN'), ('aio', 'NN'), ('jsc', 'NN'), ('nasa', 'JJ'), ('gov', 'NN'), ('apr', 'NN'), ('tm', 'NN'), ('lerc', 'NN'), ('nasa', 'JJ'), ('gov', 'NN'), ('ralibinnc', 'NN'), ('f', 'NN'), ('cbl', 'NN'), ('umd', 'JJ'), ('edu', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk refer/NN apr/NN aio/NN jsc/NN nasa/JJ)\n",
      "  (mychunk gov/NN apr/NN tm/NN lerc/NN nasa/JJ)\n",
      "  (mychunk gov/NN ralibinnc/NN f/NN cbl/NN umd/JJ)\n",
      "  (mychunk edu/NN))\n",
      "\n",
      "\n",
      "['refer', 'apr', 'aio', 'jsc', 'nasa', 'gov', 'apr', 'tm', 'lerc', 'nasa', 'gov', 'ralibinnc', 'f', 'cbl', 'umd', 'edu']\n",
      "N_grams\n",
      "4-gram:  ['refer apr aio jsc', 'apr aio jsc nasa', 'aio jsc nasa gov', 'jsc nasa gov apr', 'nasa gov apr tm', 'gov apr tm lerc', 'apr tm lerc nasa', 'tm lerc nasa gov', 'lerc nasa gov ralibinnc', 'nasa gov ralibinnc f', 'gov ralibinnc f cbl', 'ralibinnc f cbl umd', 'f cbl umd edu']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['organization', 'u', 'of', 'toronto', 'zoology']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['organization', 'u', 'toronto', 'zoology']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['organization', 'u', 'toronto', 'zoology']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['organ', 'u', 'toronto', 'zoolog']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['organ', 'u', 'toronto', 'zoolog']\n",
      "After pos_tags: [('organ', 'JJ'), ('u', 'JJ'), ('toronto', 'NN'), ('zoolog', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk organ/JJ u/JJ) (mychunk toronto/NN zoolog/NN))\n",
      "\n",
      "\n",
      "['organ', 'u', 'toronto', 'zoolog']\n",
      "N_grams\n",
      "4-gram:  ['organ u toronto zoolog']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['lines']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['lines']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['line']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['line']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['line']\n",
      "After pos_tags: [('line', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk line/NN))\n",
      "\n",
      "\n",
      "['line']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['in', 'article', 'ralibinnc', 'f', 'cbl', 'umd', 'edu', 'mike', 'starburst', 'umd', 'edu', 'michael', 'f', 'santangelo', 'writes']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['article', 'ralibinnc', 'f', 'cbl', 'umd', 'edu', 'mike', 'starburst', 'umd', 'edu', 'michael', 'f', 'santangelo', 'writes']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['article', 'ralibinnc', 'f', 'cbl', 'umd', 'edu', 'mike', 'starburst', 'umd', 'edu', 'michael', 'f', 'santangelo', 'writes']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['articl', 'ralibinnc', 'f', 'cbl', 'umd', 'edu', 'mike', 'starburst', 'umd', 'edu', 'michael', 'f', 'santangelo', 'write']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['articl', 'ralibinnc', 'f', 'cbl', 'umd', 'edu', 'mike', 'starburst', 'umd', 'edu', 'michael', 'f', 'santangelo', 'write']\n",
      "After pos_tags: [('articl', 'NN'), ('ralibinnc', 'NN'), ('f', 'NN'), ('cbl', 'NN'), ('umd', 'JJ'), ('edu', 'NN'), ('mike', 'NN'), ('starburst', 'JJ'), ('umd', 'JJ'), ('edu', 'NN'), ('michael', 'NN'), ('f', 'VBP'), ('santangelo', 'JJ'), ('write', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk articl/NN ralibinnc/NN f/NN cbl/NN umd/JJ)\n",
      "  (mychunk edu/NN mike/NN starburst/JJ umd/JJ)\n",
      "  (mychunk edu/NN michael/NN)\n",
      "  f/VBP\n",
      "  (mychunk santangelo/JJ)\n",
      "  (mychunk write/NN))\n",
      "\n",
      "\n",
      "['articl', 'ralibinnc', 'f', 'cbl', 'umd', 'edu', 'mike', 'starburst', 'umd', 'edu', 'michael', 'f', 'santangelo', 'write']\n",
      "N_grams\n",
      "4-gram:  ['articl ralibinnc f cbl', 'ralibinnc f cbl umd', 'f cbl umd edu', 'cbl umd edu mike', 'umd edu mike starburst', 'edu mike starburst umd', 'mike starburst umd edu', 'starburst umd edu michael', 'umd edu michael f', 'edu michael f santangelo', 'michael f santangelo write']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['the', 'only', 'thing']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['thing']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['thing']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['thing']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['thing']\n",
      "After pos_tags: [('thing', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk thing/NN))\n",
      "\n",
      "\n",
      "['thing']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['that', 'scares', 'me', 'is', 'the', 'part', 'about', 'simply', 'strapping', 'ssme', 's', 'and']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['scares', 'part', 'simply', 'strapping', 'ssme']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['scare', 'part', 'simply', 'strapping', 'ssme']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['scare', 'part', 'simpli', 'strap', 'ssme']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['scare', 'part', 'simpli', 'strap', 'ssme']\n",
      "After pos_tags: [('scare', 'JJ'), ('part', 'NN'), ('simpli', 'NN'), ('strap', 'NN'), ('ssme', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk scare/JJ) (mychunk part/NN simpli/NN strap/NN ssme/NN))\n",
      "\n",
      "\n",
      "['scare', 'part', 'simpli', 'strap', 'ssme']\n",
      "N_grams\n",
      "4-gram:  ['scare part simpli strap', 'part simpli strap ssme']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['a', 'nosecone', 'on', 'it', 'and', 'just', 'launching', 'it', 'i', 'have', 'this', 'vision']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['nosecone', 'launching', 'vision']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['nosecone', 'launching', 'vision']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['nosecon', 'launch', 'vision']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['nosecon', 'launch', 'vision']\n",
      "After pos_tags: [('nosecon', 'JJ'), ('launch', 'NN'), ('vision', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S (mychunk nosecon/JJ) (mychunk launch/NN vision/NN))\n",
      "\n",
      "\n",
      "['nosecon', 'launch', 'vision']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['of', 'something', 'going', 'terribly', 'wrong', 'with', 'the', 'launch', 'resulting', 'in', 'the']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['something', 'going', 'terribly', 'wrong', 'launch', 'resulting']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['something', 'going', 'terribly', 'wrong', 'launch', 'resulting']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['someth', 'go', 'terribl', 'wrong', 'launch', 'result']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['someth', 'go', 'terribl', 'wrong', 'launch', 'result']\n",
      "After pos_tags: [('someth', 'NNS'), ('go', 'VBP'), ('terribl', 'RB'), ('wrong', 'JJ'), ('launch', 'NN'), ('result', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk someth/NNS)\n",
      "  go/VBP\n",
      "  terribl/RB\n",
      "  (mychunk wrong/JJ)\n",
      "  (mychunk launch/NN result/NN))\n",
      "\n",
      "\n",
      "['someth', 'go', 'terribl', 'wrong', 'launch', 'result']\n",
      "N_grams\n",
      "4-gram:  ['someth go terribl wrong', 'go terribl wrong launch', 'terribl wrong launch result']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['complete', 'loss', 'of', 'the', 'new', 'modular', 'space', 'station', 'not', 'just', 'a', 'peice', 'of']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['complete', 'loss', 'new', 'modular', 'space', 'station', 'peice']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['complete', 'loss', 'new', 'modular', 'space', 'station', 'peice']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['complet', 'loss', 'new', 'modular', 'space', 'station', 'peic']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['complet', 'loss', 'new', 'modular', 'space', 'station', 'peic']\n",
      "After pos_tags: [('complet', 'NN'), ('loss', 'NN'), ('new', 'JJ'), ('modular', 'JJ'), ('space', 'NN'), ('station', 'NN'), ('peic', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk complet/NN loss/NN new/JJ modular/JJ)\n",
      "  (mychunk space/NN station/NN peic/NN))\n",
      "\n",
      "\n",
      "['complet', 'loss', 'new', 'modular', 'space', 'station', 'peic']\n",
      "N_grams\n",
      "4-gram:  ['complet loss new modular', 'loss new modular space', 'new modular space station', 'modular space station peic']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['it', 'as', 'would', 'be', 'the', 'case', 'with', 'staged', 'in', 'orbit', 'construction']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['would', 'case', 'staged', 'orbit', 'construction']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['would', 'case', 'staged', 'orbit', 'construction']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['would', 'case', 'stage', 'orbit', 'construct']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['would', 'case', 'stage', 'orbit', 'construct']\n",
      "After pos_tags: [('would', 'MD'), ('case', 'NN'), ('stage', 'VB'), ('orbit', 'NN'), ('construct', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  would/MD\n",
      "  (mychunk case/NN)\n",
      "  stage/VB\n",
      "  (mychunk orbit/NN construct/NN))\n",
      "\n",
      "\n",
      "['would', 'case', 'stage', 'orbit', 'construct']\n",
      "N_grams\n",
      "4-gram:  ['would case stage orbit', 'case stage orbit construct']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['it', 'doesn', 't', 'make', 'a', 'whole', 'lot', 'of', 'difference', 'actually', 'since', 'they', 'weren', 't']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['make', 'whole', 'lot', 'difference', 'actually', 'since']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['make', 'whole', 'lot', 'difference', 'actually', 'since']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['make', 'whole', 'lot', 'differ', 'actual', 'sinc']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['make', 'whole', 'lot', 'differ', 'actual', 'sinc']\n",
      "After pos_tags: [('make', 'VB'), ('whole', 'JJ'), ('lot', 'NN'), ('differ', 'VBP'), ('actual', 'JJ'), ('sinc', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  make/VB\n",
      "  (mychunk whole/JJ)\n",
      "  (mychunk lot/NN)\n",
      "  differ/VBP\n",
      "  (mychunk actual/JJ)\n",
      "  (mychunk sinc/NN))\n",
      "\n",
      "\n",
      "['make', 'whole', 'lot', 'differ', 'actual', 'sinc']\n",
      "N_grams\n",
      "4-gram:  ['make whole lot differ', 'whole lot differ actual', 'lot differ actual sinc']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['building', 'spares', 'of', 'the', 'station', 'hardware', 'anyway', 'dumb', 'at', 'least', 'this']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['building', 'spares', 'station', 'hardware', 'anyway', 'dumb', 'least']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['building', 'spare', 'station', 'hardware', 'anyway', 'dumb', 'least']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['build', 'spare', 'station', 'hardwar', 'anyway', 'dumb', 'least']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['build', 'spare', 'station', 'hardwar', 'anyway', 'dumb', 'least']\n",
      "After pos_tags: [('build', 'NN'), ('spare', 'JJ'), ('station', 'NN'), ('hardwar', 'VBD'), ('anyway', 'RB'), ('dumb', 'JJ'), ('least', 'JJS')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk build/NN spare/JJ)\n",
      "  (mychunk station/NN hardwar/VBD)\n",
      "  anyway/RB\n",
      "  (mychunk dumb/JJ least/JJS))\n",
      "\n",
      "\n",
      "['build', 'spare', 'station', 'hardwar', 'anyway', 'dumb', 'least']\n",
      "N_grams\n",
      "4-gram:  ['build spare station hardwar', 'spare station hardwar anyway', 'station hardwar anyway dumb', 'hardwar anyway dumb least']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['is', 'only', 'one', 'launch', 'to', 'fail']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['one', 'launch', 'fail']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['one', 'launch', 'fail']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['one', 'launch', 'fail']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['one', 'launch', 'fail']\n",
      "After pos_tags: [('one', 'CD'), ('launch', 'NN'), ('fail', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S one/CD (mychunk launch/NN fail/NN))\n",
      "\n",
      "\n",
      "['one', 'launch', 'fail']\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "[]\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "[]\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "[]\n",
      "\n",
      "\n",
      "Stemming\n",
      "[]\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: []\n",
      "After pos_tags: []\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "Warning: parsing empty text\n",
      "After Chunking (S )\n",
      "\n",
      "\n",
      "[]\n",
      "N_grams\n",
      "4-gram:  []\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['svr', 'resembles', 'a', 'high', 'speed', 'collision', 'henry', 'spencer', 'u', 'of', 'toronto', 'zoology']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['svr', 'resembles', 'high', 'speed', 'collision', 'henry', 'spencer', 'u', 'toronto', 'zoology']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['svr', 'resembles', 'high', 'speed', 'collision', 'henry', 'spencer', 'u', 'toronto', 'zoology']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['svr', 'resembl', 'high', 'speed', 'collis', 'henri', 'spencer', 'u', 'toronto', 'zoolog']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['svr', 'resembl', 'high', 'speed', 'collis', 'henri', 'spencer', 'u', 'toronto', 'zoolog']\n",
      "After pos_tags: [('svr', 'NN'), ('resembl', 'NN'), ('high', 'JJ'), ('speed', 'NN'), ('collis', 'NNS'), ('henri', 'VBP'), ('spencer', 'NN'), ('u', 'JJ'), ('toronto', 'NN'), ('zoolog', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk svr/NN resembl/NN high/JJ)\n",
      "  (mychunk speed/NN collis/NNS)\n",
      "  henri/VBP\n",
      "  (mychunk spencer/NN u/JJ)\n",
      "  (mychunk toronto/NN zoolog/NN))\n",
      "\n",
      "\n",
      "['svr', 'resembl', 'high', 'speed', 'collis', 'henri', 'spencer', 'u', 'toronto', 'zoolog']\n",
      "N_grams\n",
      "4-gram:  ['svr resembl high speed', 'resembl high speed collis', 'high speed collis henri', 'speed collis henri spencer', 'collis henri spencer u', 'henri spencer u toronto', 'spencer u toronto zoolog']\n",
      "\n",
      "\n",
      "tokenized words\n",
      "['between', 'svr', 'and', 'sunos', 'dick', 'dunn', 'henry', 'zoo', 'toronto', 'edu', 'utzoo', 'henry']\n",
      "\n",
      "\n",
      "Removing Stopwords\n",
      "['svr', 'sunos', 'dick', 'dunn', 'henry', 'zoo', 'toronto', 'edu', 'utzoo', 'henry']\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "['svr', 'sunos', 'dick', 'dunn', 'henry', 'zoo', 'toronto', 'edu', 'utzoo', 'henry']\n",
      "\n",
      "\n",
      "Stemming\n",
      "['svr', 'suno', 'dick', 'dunn', 'henri', 'zoo', 'toronto', 'edu', 'utzoo', 'henri']\n",
      "\n",
      "\n",
      "pos_tags and chunking\n",
      "After Split: ['svr', 'suno', 'dick', 'dunn', 'henri', 'zoo', 'toronto', 'edu', 'utzoo', 'henri']\n",
      "After pos_tags: [('svr', 'NN'), ('suno', 'NN'), ('dick', 'NN'), ('dunn', 'NN'), ('henri', 'NN'), ('zoo', 'NN'), ('toronto', 'IN'), ('edu', 'JJ'), ('utzoo', 'JJ'), ('henri', 'NN')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk svr/NN suno/NN dick/NN dunn/NN henri/NN zoo/NN)\n",
      "  toronto/IN\n",
      "  (mychunk edu/JJ utzoo/JJ)\n",
      "  (mychunk henri/NN))\n",
      "\n",
      "\n",
      "['svr', 'suno', 'dick', 'dunn', 'henri', 'zoo', 'toronto', 'edu', 'utzoo', 'henri']\n",
      "N_grams\n",
      "4-gram:  ['svr suno dick dunn', 'suno dick dunn henri', 'dick dunn henri zoo', 'dunn henri zoo toronto', 'henri zoo toronto edu', 'zoo toronto edu utzoo', 'toronto edu utzoo henri']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(line, num):\n",
    "    n_grams = ngrams(line, num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    line=[]\n",
    "    line=re.sub('[^a-zA-Z]',' ',lines[i])\n",
    "    line = line.lower()\n",
    "    line = line.split()#tokenization \n",
    "    print(\"tokenized words\")\n",
    "    print(line)\n",
    "    print(\"\\n\")\n",
    "    line=[word for word in line if word not in set(stopwords.words('english'))]\n",
    "    print(\"Removing Stopwords\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [lemmatizer.lemmatize(word) for word in line]\n",
    "    print(\"Lemmatization\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    line = [ps.stem(word) for word in line]\n",
    "    print(\"Stemming\")\n",
    "    print(line)\n",
    "    print('\\n')\n",
    "    print(\"pos_tags and chunking\")\n",
    "    print(\"After Split:\",line)\n",
    "    tokens_tag = pos_tag(line)\n",
    "    print(\"After pos_tags:\",tokens_tag)\n",
    "    patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "    chunker = RegexpParser(patterns)\n",
    "    print(\"After Regex:\",chunker)\n",
    "    output = chunker.parse(tokens_tag)\n",
    "    print(\"After Chunking\",output)\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(line)\n",
    "    print(\"N_grams\")\n",
    "    print(\"4-gram: \", extract_ngrams(line, 4))\n",
    "    print('\\n')\n",
    "    \n",
    "    line= [word for word in line if word in words]#taking dictionary words only to build final bag of word models\n",
    "    line = [word for word in line if len(word)>1]\n",
    "\n",
    "\n",
    "\n",
    "    line=' '.join(line)\n",
    "    if(line):\n",
    "        corpus.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt graphic alt alt erotica',\n",
       " 'alt graphic alt alt erotica',\n",
       " 'path state oar net sol white',\n",
       " 'white gum',\n",
       " 'subject answer',\n",
       " 'refer tue',\n",
       " 'organ white',\n",
       " 'date',\n",
       " 'tin',\n",
       " 'id white',\n",
       " 'line',\n",
       " 'renew blade stack tue walter write',\n",
       " 'kind soul mail bug',\n",
       " 'posit post ask post',\n",
       " 'name come',\n",
       " 'quit number bug one mention',\n",
       " 'stupid question run get',\n",
       " 'still need gif viewer without window',\n",
       " 'thank',\n",
       " 'auto',\n",
       " 'path da news near net an net net',\n",
       " 'nu nation',\n",
       " 'subject warn read',\n",
       " 'id nu',\n",
       " 'organ nation',\n",
       " 'tin',\n",
       " 'refer news',\n",
       " 'date wed',\n",
       " 'line',\n",
       " 'write',\n",
       " 'mani',\n",
       " 'steal car els',\n",
       " 'mention commit senseless act rape cannot',\n",
       " 'term money human be submit',\n",
       " 'human be think right track',\n",
       " 'scratch car bother death',\n",
       " 'number scum feel way admit',\n",
       " 'go mere fact flesh',\n",
       " 'move shape human',\n",
       " 'mean right',\n",
       " 'art space',\n",
       " 'art space',\n",
       " 'path magnesium club news state state destroy ca ca annex ca palmer',\n",
       " 'palmer palmer ca',\n",
       " 'subject drive vacuum',\n",
       " 'id ca',\n",
       " 'id annex ca',\n",
       " 'sender news ca',\n",
       " 'organ',\n",
       " 'refer net',\n",
       " 'date sat',\n",
       " 'line',\n",
       " 'net net',\n",
       " 'write',\n",
       " 'background concept',\n",
       " 'concept serious',\n",
       " 'freeman al mani year ago know well known',\n",
       " 'high flew san',\n",
       " 'back work atom time',\n",
       " 'learn almost year later',\n",
       " 'ted visit reveal done feel sure',\n",
       " 'must film like',\n",
       " 'see seen']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging abbreviations\n",
    "# Abbreviation\tMeaning\n",
    "# CC\tcoordinating conjunction\n",
    "# CD\tcardinal digit\n",
    "# DT\tdeterminer\n",
    "# EX\texistential there\n",
    "# FW\tforeign word\n",
    "# IN\tpreposition/subordinating conjunction\n",
    "# JJ\tadjective (large)\n",
    "# JJR\tadjective, comparative (larger)\n",
    "# JJS\tadjective, superlative (largest)\n",
    "# LS\tlist market\n",
    "# MD\tmodal (could, will)\n",
    "# NN\tnoun, singular (cat, tree)\n",
    "# NNS\tnoun plural (desks)\n",
    "# NNP\tproper noun, singular (sarah)\n",
    "# NNPS\tproper noun, plural (indians or americans)\n",
    "# PDT\tpredeterminer (all, both, half)\n",
    "# POS\tpossessive ending (parent\\ 's)\n",
    "# PRP\tpersonal pronoun (hers, herself, him,himself)\n",
    "# PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "# RB\tadverb (occasionally, swiftly)\n",
    "# RBR\tadverb, comparative (greater)\n",
    "# RBS\tadverb, superlative (biggest)\n",
    "# RP\tparticle (about)\n",
    "# TO\tinfinite marker (to)\n",
    "# UH\tinterjection (goodbye)\n",
    "# VB\tverb (ask)\n",
    "# VBG\tverb gerund (judging)\n",
    "# VBD\tverb past tense (pleaded)\n",
    "# VBN\tverb past participle (reunified)\n",
    "# VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "# VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "# WDT\twh-determiner (that, what)\n",
    "# WP\twh- pronoun (who)\n",
    "# WRB\twh- adverb (how)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
